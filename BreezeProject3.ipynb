{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc27289-1262-4a04-9693-f4555ee10b9f",
   "metadata": {},
   "source": [
    "# Project 3 - Supervised Learning and Modeling  \n",
    "\n",
    "Kelley Breeze\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ac5e8-0fd1-4b5e-bf9b-da434b80a6ee",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "\n",
    "In this project we will walk through how to use supervised learning to build models in order to predict responses from our dataset. Before we get started let's go over a little bit about what supervised learning is, what we want to do with our models, and briefly discuss the dataset that we will be using throughout the project.\n",
    "\n",
    "### Supervised Learning  \n",
    "In supervised learning you have a response variable that you are trying to predict. If we fit a predictive model using supervised learning it will be possible to evaluate how well our model predicts the value of our response variable by using observations that were not used in fitting our model.\n",
    "\n",
    "### Absenteeism At Work Dataset  \n",
    "\n",
    "The dataset that we will use contains information about absenteeism at a courier company in Brazil from July 2007 to July 2010. This dataset is from the UC Irvine Machine Learning Repository and can be found [here](https://archive-beta.ics.uci.edu/dataset/445/absenteeism+at+work). It contains 21 variables that are detailed below:  \n",
    "\n",
    "1.  `ID` - **Categorical** - Individual Identification - There are 36 unique employees in the dataset. *This variable will be dropped prior to model building*\n",
    "2.  `Reason for absence` - **Categorical** - This is the recorded reason for the employee's absence. This is a categorical variable with 28 total levels that has been coded numerically. The values 1-21, corresponding to I through XXI, are taken from the World Health Organization's [International Statistical Classification of Diseases and Related Helth Problems 10 Revision](https://icd.who.int/browse10/2010/en#/). The remaining 7 values of this variable are:  \n",
    "    -  `22` - patient follow-up  \n",
    "    -  `23` - medical consultaion  \n",
    "    -  `24` - blood donation  \n",
    "    -  `25` - laboratory examination  \n",
    "    -  `26` - unjustified absence  \n",
    "    -  `27` - phisiotherapy  \n",
    "    -  `28` - dental consulation  \n",
    "    \n",
    "    Note that there are a total of 43 records for which the `Reason for absence` is recorded as `0`. We will simply refer to these as representing a value of `unkonwn` for our `Reason for absence` variable.  \n",
    "3.  `Month of absence` - **Categorical** - This is the month in which the absence is registered.  \n",
    "4.  `Day of the week` - **Categorical** - Work day (Monday through Friday) - Categorical - this is coded numerically with the following values:  \n",
    "    -  `2` - Monday  \n",
    "    -  `3` - Tuesday  \n",
    "    -  `4` - Wednesday  \n",
    "    -  `5` - Thursday  \n",
    "    -  `6` - Friday  \n",
    "5.  `Seasons` - **Categorical** - coded numerically with the following values:  \n",
    "    -  `1` - summer  \n",
    "    -  `2` - autumn  \n",
    "    -  `3` - winter  \n",
    "    -  `4` - spring  \n",
    "6. `Transportation expense` - **Numeric** -  This is the monthly transportation expense of each employee in dollars\n",
    "7. `Distance from Residence to Work` - **Numeric** - This is the distance in kilometers that the employee must travel each day to get to work measured in kilometers.    \n",
    "8. `Service time`  - **Numeric** - the service time of each employee in years.  \n",
    "9.  `Age` - **Numeric** - Age of employee in years.  \n",
    "10. `Work load Average/day` - **Numeric** - This is the average workload per day for the employee, units unknown.  \n",
    "11. `Hit target` - **Numeric** - This is an achievement percentage for periodic goals for each employee.  \n",
    "12. `Disciplinary failure (yes=1; no=0)` - **Categorical** - Binary yes/no about whether the employee recieved a disciplinary warning that month.   \n",
    "13. `Education` - **Categorical/Ordinal** - The highest education level attained by the employee, coded numerically with the following values:  \n",
    "    -  `1` - high school  \n",
    "    -  `2` - graduate  \n",
    "    -  `3` - postgraduate  \n",
    "    -  `4` - master and doctor  \n",
    "14. `Son` - **Numeric** - The total number of children of the employee.  \n",
    "15. `Social drinker` - **Categorical** - This is a binary variable where yes = `1` indicates that the employee is a social drinker and no = `0` indicates that the employee is not a social drinker.  \n",
    "16. `Social smoker`- **Categorical** - This is a binary variable where yes = `1` indicates that the employee is a social smoker and no = `0` indicates that the employee is not a social smoker.    \n",
    "17. `Pet` - **Numeric** - This is the number of pets owned by the employee.  \n",
    "18. `Weight` - ***Numeric** - The employee's weight in kilograms.  \n",
    "19. `Height` - **Numeric** - The employee's height in centimeters.  \n",
    "20. `Body mass index` - **Numeric** - The employee's body mass index. *This variable will be dropped as it is highly correlated with weight and height*   \n",
    "21. `Absenteeism time in hours` - **Numeric** - This is our target variable. `Absenteeism time in hours` is a continuous numeric variable representing the number of hours that an employee was absent for a given instance of missed work.  \n",
    "\n",
    "### Question that we want to answer  \n",
    "\n",
    "Can we predict the number of hours that an employee will be absent based on the personal and work related information for that employee provided in the dataset?  \n",
    "\n",
    "### Modeling Goals  \n",
    "\n",
    "Our goal in working with this dataset is to build and test multiple models to predict the number of absentee hours (variable 21) for an employee based on a subset of the available variables (1-20) listed above. We will use five different modeling techniques to build, train, and test models based on the absenteeism at work dataset. We will then compare our models to one another to determine which model is the best at predicting our response variable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d34e3c-e4dc-499b-8848-b9ff2bb02e6a",
   "metadata": {},
   "source": [
    "## Starting Spark Session and Reading in Our Data  \n",
    "\n",
    "We will begin by starting our spark session and reading in our data. We will also import `pandas` and `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4b58ed-da20-46b6-b44d-6b7e7ab505b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0f7a4cb7-f701-44c2-8f48-59c664c2a98d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason for absence</th>\n",
       "      <th>Month of absence</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation expense</th>\n",
       "      <th>Distance from Residence to Work</th>\n",
       "      <th>Service time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Work load Average/day</th>\n",
       "      <th>...</th>\n",
       "      <th>Disciplinary failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Son</th>\n",
       "      <th>Social drinker</th>\n",
       "      <th>Social smoker</th>\n",
       "      <th>Pet</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Body mass index</th>\n",
       "      <th>Absenteeism time in hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>172</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>168</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>172</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Reason for absence  Month of absence  Day of the week  Seasons  \\\n",
       "0  11                  26                 7                3        1   \n",
       "1  36                   0                 7                3        1   \n",
       "2   3                  23                 7                4        1   \n",
       "3   7                   7                 7                5        1   \n",
       "4  11                  23                 7                5        1   \n",
       "\n",
       "   Transportation expense  Distance from Residence to Work  Service time  Age  \\\n",
       "0                     289                               36            13   33   \n",
       "1                     118                               13            18   50   \n",
       "2                     179                               51            18   38   \n",
       "3                     279                                5            14   39   \n",
       "4                     289                               36            13   33   \n",
       "\n",
       "   Work load Average/day  ...  Disciplinary failure  Education  Son  \\\n",
       "0                239.554  ...                     0          1    2   \n",
       "1                239.554  ...                     1          1    1   \n",
       "2                239.554  ...                     0          1    0   \n",
       "3                239.554  ...                     0          1    2   \n",
       "4                239.554  ...                     0          1    2   \n",
       "\n",
       "   Social drinker  Social smoker  Pet  Weight  Height  Body mass index  \\\n",
       "0               1              0    1      90     172               30   \n",
       "1               1              0    0      98     178               31   \n",
       "2               1              0    0      89     170               31   \n",
       "3               1              1    0      68     168               24   \n",
       "4               1              0    1      90     172               30   \n",
       "\n",
       "   Absenteeism time in hours  \n",
       "0                          4  \n",
       "1                          0  \n",
       "2                          2  \n",
       "3                          4  \n",
       "4                          2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the data\n",
    "absentee_data_pd = pd.read_csv('Absenteeism_at_work.csv', sep = ';')\n",
    "\n",
    "# Using the head() method to make sure everything looks as expected\n",
    "absentee_data_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59201bb4-8d1b-4de9-8034-e03bc7c8d96b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Formatting Column Names  \n",
    "\n",
    "The original dataset contains variable names with spaces. To make things easier let's replace the spaces in any of the column names with `_` to prevent issues that might arise from the spaces being present in our names. This can easily be done using `str.replace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "847c64e6-022c-4a57-ba07-eb0ea4345e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason_for_absence</th>\n",
       "      <th>Month_of_absence</th>\n",
       "      <th>Day_of_the_week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation_expense</th>\n",
       "      <th>Distance_from_Residence_to_Work</th>\n",
       "      <th>Service_time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Work_load_Average/day</th>\n",
       "      <th>...</th>\n",
       "      <th>Disciplinary_failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Son</th>\n",
       "      <th>Social_drinker</th>\n",
       "      <th>Social_smoker</th>\n",
       "      <th>Pet</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Body_mass_index</th>\n",
       "      <th>Absenteeism_time_in_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>172</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>168</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>172</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Reason_for_absence  Month_of_absence  Day_of_the_week  Seasons  \\\n",
       "0  11                  26                 7                3        1   \n",
       "1  36                   0                 7                3        1   \n",
       "2   3                  23                 7                4        1   \n",
       "3   7                   7                 7                5        1   \n",
       "4  11                  23                 7                5        1   \n",
       "\n",
       "   Transportation_expense  Distance_from_Residence_to_Work  Service_time  Age  \\\n",
       "0                     289                               36            13   33   \n",
       "1                     118                               13            18   50   \n",
       "2                     179                               51            18   38   \n",
       "3                     279                                5            14   39   \n",
       "4                     289                               36            13   33   \n",
       "\n",
       "   Work_load_Average/day  ...  Disciplinary_failure  Education  Son  \\\n",
       "0                239.554  ...                     0          1    2   \n",
       "1                239.554  ...                     1          1    1   \n",
       "2                239.554  ...                     0          1    0   \n",
       "3                239.554  ...                     0          1    2   \n",
       "4                239.554  ...                     0          1    2   \n",
       "\n",
       "   Social_drinker  Social_smoker  Pet  Weight  Height  Body_mass_index  \\\n",
       "0               1              0    1      90     172               30   \n",
       "1               1              0    0      98     178               31   \n",
       "2               1              0    0      89     170               31   \n",
       "3               1              1    0      68     168               24   \n",
       "4               1              0    1      90     172               30   \n",
       "\n",
       "   Absenteeism_time_in_hours  \n",
       "0                          4  \n",
       "1                          0  \n",
       "2                          2  \n",
       "3                          4  \n",
       "4                          2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformatting variable names to replace spaces with _\n",
    "absentee_data_pd.columns = absentee_data_pd.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "# Using the head() method to make sure everything looks as expected\n",
    "absentee_data_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa43b1-3de7-4886-a32e-17a1a91ea2cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A Quick Look at the Data  \n",
    "\n",
    "Before we get into the model building phase, let's take a quick look at our data to see how the variables are stored and looking at some basic statistics about the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ba3feef-01bd-40d5-bbf6-6323c0e339e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 740 entries, 0 to 739\n",
      "Data columns (total 21 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   ID                               740 non-null    int64  \n",
      " 1   Reason for absence               740 non-null    int64  \n",
      " 2   Month of absence                 740 non-null    int64  \n",
      " 3   Day of the week                  740 non-null    int64  \n",
      " 4   Seasons                          740 non-null    int64  \n",
      " 5   Transportation expense           740 non-null    int64  \n",
      " 6   Distance from Residence to Work  740 non-null    int64  \n",
      " 7   Service time                     740 non-null    int64  \n",
      " 8   Age                              740 non-null    int64  \n",
      " 9   Work load Average/day            740 non-null    float64\n",
      " 10  Hit target                       740 non-null    int64  \n",
      " 11  Disciplinary failure             740 non-null    int64  \n",
      " 12  Education                        740 non-null    int64  \n",
      " 13  Son                              740 non-null    int64  \n",
      " 14  Social drinker                   740 non-null    int64  \n",
      " 15  Social smoker                    740 non-null    int64  \n",
      " 16  Pet                              740 non-null    int64  \n",
      " 17  Weight                           740 non-null    int64  \n",
      " 18  Height                           740 non-null    int64  \n",
      " 19  Body mass index                  740 non-null    int64  \n",
      " 20  Absenteeism time in hours        740 non-null    int64  \n",
      "dtypes: float64(1), int64(20)\n",
      "memory usage: 121.5 KB\n"
     ]
    }
   ],
   "source": [
    "absentee_data_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44afa3a9-f777-4b25-a9f0-19507efa822d",
   "metadata": {},
   "source": [
    "### Summary Statistics About Our Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e4d806-e9ce-462b-b705-94eb127db6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason_for_absence</th>\n",
       "      <th>Month_of_absence</th>\n",
       "      <th>Day_of_the_week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation_expense</th>\n",
       "      <th>Distance_from_Residence_to_Work</th>\n",
       "      <th>Service_time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Work_load_Average/day_</th>\n",
       "      <th>...</th>\n",
       "      <th>Disciplinary_failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Son</th>\n",
       "      <th>Social_drinker</th>\n",
       "      <th>Social_smoker</th>\n",
       "      <th>Pet</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Body_mass_index</th>\n",
       "      <th>Absenteeism_time_in_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>740.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.017568</td>\n",
       "      <td>19.216216</td>\n",
       "      <td>6.324324</td>\n",
       "      <td>3.914865</td>\n",
       "      <td>2.544595</td>\n",
       "      <td>221.329730</td>\n",
       "      <td>29.631081</td>\n",
       "      <td>12.554054</td>\n",
       "      <td>36.450000</td>\n",
       "      <td>271.490235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>1.291892</td>\n",
       "      <td>1.018919</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.072973</td>\n",
       "      <td>0.745946</td>\n",
       "      <td>79.035135</td>\n",
       "      <td>172.114865</td>\n",
       "      <td>26.677027</td>\n",
       "      <td>6.924324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.021247</td>\n",
       "      <td>8.433406</td>\n",
       "      <td>3.436287</td>\n",
       "      <td>1.421675</td>\n",
       "      <td>1.111831</td>\n",
       "      <td>66.952223</td>\n",
       "      <td>14.836788</td>\n",
       "      <td>4.384873</td>\n",
       "      <td>6.478772</td>\n",
       "      <td>39.058116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226277</td>\n",
       "      <td>0.673238</td>\n",
       "      <td>1.098489</td>\n",
       "      <td>0.495749</td>\n",
       "      <td>0.260268</td>\n",
       "      <td>1.318258</td>\n",
       "      <td>12.883211</td>\n",
       "      <td>6.034995</td>\n",
       "      <td>4.285452</td>\n",
       "      <td>13.330998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>205.917000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>244.387000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>264.249000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>294.217000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>378.884000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  Reason_for_absence  Month_of_absence  Day_of_the_week  \\\n",
       "count  740.000000          740.000000        740.000000       740.000000   \n",
       "mean    18.017568           19.216216          6.324324         3.914865   \n",
       "std     11.021247            8.433406          3.436287         1.421675   \n",
       "min      1.000000            0.000000          0.000000         2.000000   \n",
       "25%      9.000000           13.000000          3.000000         3.000000   \n",
       "50%     18.000000           23.000000          6.000000         4.000000   \n",
       "75%     28.000000           26.000000          9.000000         5.000000   \n",
       "max     36.000000           28.000000         12.000000         6.000000   \n",
       "\n",
       "          Seasons  Transportation_expense  Distance_from_Residence_to_Work  \\\n",
       "count  740.000000              740.000000                       740.000000   \n",
       "mean     2.544595              221.329730                        29.631081   \n",
       "std      1.111831               66.952223                        14.836788   \n",
       "min      1.000000              118.000000                         5.000000   \n",
       "25%      2.000000              179.000000                        16.000000   \n",
       "50%      3.000000              225.000000                        26.000000   \n",
       "75%      4.000000              260.000000                        50.000000   \n",
       "max      4.000000              388.000000                        52.000000   \n",
       "\n",
       "       Service_time         Age  Work_load_Average/day_  ...  \\\n",
       "count    740.000000  740.000000              740.000000  ...   \n",
       "mean      12.554054   36.450000              271.490235  ...   \n",
       "std        4.384873    6.478772               39.058116  ...   \n",
       "min        1.000000   27.000000              205.917000  ...   \n",
       "25%        9.000000   31.000000              244.387000  ...   \n",
       "50%       13.000000   37.000000              264.249000  ...   \n",
       "75%       16.000000   40.000000              294.217000  ...   \n",
       "max       29.000000   58.000000              378.884000  ...   \n",
       "\n",
       "       Disciplinary_failure   Education         Son  Social_drinker  \\\n",
       "count            740.000000  740.000000  740.000000      740.000000   \n",
       "mean               0.054054    1.291892    1.018919        0.567568   \n",
       "std                0.226277    0.673238    1.098489        0.495749   \n",
       "min                0.000000    1.000000    0.000000        0.000000   \n",
       "25%                0.000000    1.000000    0.000000        0.000000   \n",
       "50%                0.000000    1.000000    1.000000        1.000000   \n",
       "75%                0.000000    1.000000    2.000000        1.000000   \n",
       "max                1.000000    4.000000    4.000000        1.000000   \n",
       "\n",
       "       Social_smoker         Pet      Weight      Height  Body_mass_index  \\\n",
       "count     740.000000  740.000000  740.000000  740.000000       740.000000   \n",
       "mean        0.072973    0.745946   79.035135  172.114865        26.677027   \n",
       "std         0.260268    1.318258   12.883211    6.034995         4.285452   \n",
       "min         0.000000    0.000000   56.000000  163.000000        19.000000   \n",
       "25%         0.000000    0.000000   69.000000  169.000000        24.000000   \n",
       "50%         0.000000    0.000000   83.000000  170.000000        25.000000   \n",
       "75%         0.000000    1.000000   89.000000  172.000000        31.000000   \n",
       "max         1.000000    8.000000  108.000000  196.000000        38.000000   \n",
       "\n",
       "       Absenteeism_time_in_hours  \n",
       "count                 740.000000  \n",
       "mean                    6.924324  \n",
       "std                    13.330998  \n",
       "min                     0.000000  \n",
       "25%                     2.000000  \n",
       "50%                     3.000000  \n",
       "75%                     8.000000  \n",
       "max                   120.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absentee_data_pd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f9bce-ad29-43e3-9e01-b1c29ab6816b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "We can see based on the information above that all of the variables are being stored as numberic type, with all except work load average per day being stored as `int64`, work load is stored as `float64`. In reality many of these variables are categorical, and we will want to address this when building our model pipeline. We can also see that our true numeric variables are on very different scales, so we will want to do some standardization before we use these in our linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b119d65-61ba-4a89-a295-a10cbf0cbaa7",
   "metadata": {},
   "source": [
    "### Converting to Spark SQL Dataframe  \n",
    "\n",
    "Spark SQL dataframes are the preferred type of dataframe to use when building pipelines in `MLlib`. We will now convert `absentee_data_pd` to a Spark SQL dataframe. We will save this as `absentee_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "30065505-14a9-46ea-9afa-6bbaabca3626",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+---------------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+\n",
      "| ID|Reason_for_absence|Month_of_absence|Day_of_the_week|Seasons|Transportation_expense|Distance_from_Residence_to_Work|Service_time|Age|Work_load_Average/day|Hit_target|Disciplinary_failure|Education|Son|Social_drinker|Social_smoker|Pet|Weight|Height|Body_mass_index|Absenteeism_time_in_hours|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+---------------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+\n",
      "| 11|                26|               7|              3|      1|                   289|                             36|          13| 33|              239.554|        97|                   0|        1|  2|             1|            0|  1|    90|   172|             30|                        4|\n",
      "| 36|                 0|               7|              3|      1|                   118|                             13|          18| 50|              239.554|        97|                   1|        1|  1|             1|            0|  0|    98|   178|             31|                        0|\n",
      "|  3|                23|               7|              4|      1|                   179|                             51|          18| 38|              239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|\n",
      "|  7|                 7|               7|              5|      1|                   279|                              5|          14| 39|              239.554|        97|                   0|        1|  2|             1|            1|  0|    68|   168|             24|                        4|\n",
      "| 11|                23|               7|              5|      1|                   289|                             36|          13| 33|              239.554|        97|                   0|        1|  2|             1|            0|  1|    90|   172|             30|                        2|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+---------------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "absentee_data = spark.createDataFrame(absentee_data_pd)\n",
    "absentee_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cc5e0-3476-432a-8525-4bc5ceba1eb9",
   "metadata": {},
   "source": [
    "Again, let's take a quick look at the data to make sure everything looks as expected. We can see that all variables are stored as `bigint`, except for the work load variable which is stored as a `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "12a9d4bf-3110-41d5-a765-bfc377e9fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: bigint, Reason_for_absence: bigint, Month_of_absence: bigint, Day_of_the_week: bigint, Seasons: bigint, Transportation_expense: bigint, Distance_from_Residence_to_Work: bigint, Service_time: bigint, Age: bigint, Work_load_Average/day: double, Hit_target: bigint, Disciplinary_failure: bigint, Education: bigint, Son: bigint, Social_drinker: bigint, Social_smoker: bigint, Pet: bigint, Weight: bigint, Height: bigint, Body_mass_index: bigint, Absenteeism_time_in_hours: bigint]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absentee_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26c1bd-3555-4b32-aea4-705d6eb7d11c",
   "metadata": {},
   "source": [
    "Before moving on, let's rename the `Work_load_Average/day` variable to `Workload_per_day` to prevent issues caused by the `/` in the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1461bace-a1eb-48a7-a87d-0d369488e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "absentee_data = absentee_data.withColumnRenamed(\"Work_load_Average/day\", \"Workload_per_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6098e4d1-5ca9-42de-8b57-aed7f2670106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>740</td>\n",
       "      <td>18.017567567567568</td>\n",
       "      <td>11.021247263063655</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reason_for_absence</th>\n",
       "      <td>740</td>\n",
       "      <td>19.216216216216218</td>\n",
       "      <td>8.433405882799654</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_of_absence</th>\n",
       "      <td>740</td>\n",
       "      <td>6.324324324324325</td>\n",
       "      <td>3.4362869319125893</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day_of_the_week</th>\n",
       "      <td>740</td>\n",
       "      <td>3.9148648648648647</td>\n",
       "      <td>1.4216747097562803</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seasons</th>\n",
       "      <td>740</td>\n",
       "      <td>2.5445945945945945</td>\n",
       "      <td>1.111831060157382</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transportation_expense</th>\n",
       "      <td>740</td>\n",
       "      <td>221.32972972972973</td>\n",
       "      <td>66.95222324531973</td>\n",
       "      <td>118</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Distance_from_Residence_to_Work</th>\n",
       "      <td>740</td>\n",
       "      <td>29.63108108108108</td>\n",
       "      <td>14.836788436739145</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service_time</th>\n",
       "      <td>740</td>\n",
       "      <td>12.554054054054054</td>\n",
       "      <td>4.384873407621149</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>740</td>\n",
       "      <td>36.45</td>\n",
       "      <td>6.47877245761187</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Workload_per_day</th>\n",
       "      <td>740</td>\n",
       "      <td>271.4902351351353</td>\n",
       "      <td>39.05811618814401</td>\n",
       "      <td>205.917</td>\n",
       "      <td>378.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hit_target</th>\n",
       "      <td>740</td>\n",
       "      <td>94.58783783783784</td>\n",
       "      <td>3.7793131344179947</td>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disciplinary_failure</th>\n",
       "      <td>740</td>\n",
       "      <td>0.05405405405405406</td>\n",
       "      <td>0.22627727323215055</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>740</td>\n",
       "      <td>1.2918918918918918</td>\n",
       "      <td>0.6732380415251594</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Son</th>\n",
       "      <td>740</td>\n",
       "      <td>1.018918918918919</td>\n",
       "      <td>1.0984890195302819</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Social_drinker</th>\n",
       "      <td>740</td>\n",
       "      <td>0.5675675675675675</td>\n",
       "      <td>0.495748667200035</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Social_smoker</th>\n",
       "      <td>740</td>\n",
       "      <td>0.07297297297297298</td>\n",
       "      <td>0.2602680502800184</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pet</th>\n",
       "      <td>740</td>\n",
       "      <td>0.745945945945946</td>\n",
       "      <td>1.3182582913258338</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>740</td>\n",
       "      <td>79.03513513513514</td>\n",
       "      <td>12.883210507177221</td>\n",
       "      <td>56</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height</th>\n",
       "      <td>740</td>\n",
       "      <td>172.11486486486487</td>\n",
       "      <td>6.03499453026766</td>\n",
       "      <td>163</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body_mass_index</th>\n",
       "      <td>740</td>\n",
       "      <td>26.677027027027027</td>\n",
       "      <td>4.285452223167275</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Absenteeism_time_in_hours</th>\n",
       "      <td>740</td>\n",
       "      <td>6.924324324324324</td>\n",
       "      <td>13.330998100978201</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0                    1  \\\n",
       "summary                          count                 mean   \n",
       "ID                                 740   18.017567567567568   \n",
       "Reason_for_absence                 740   19.216216216216218   \n",
       "Month_of_absence                   740    6.324324324324325   \n",
       "Day_of_the_week                    740   3.9148648648648647   \n",
       "Seasons                            740   2.5445945945945945   \n",
       "Transportation_expense             740   221.32972972972973   \n",
       "Distance_from_Residence_to_Work    740    29.63108108108108   \n",
       "Service_time                       740   12.554054054054054   \n",
       "Age                                740                36.45   \n",
       "Workload_per_day                   740    271.4902351351353   \n",
       "Hit_target                         740    94.58783783783784   \n",
       "Disciplinary_failure               740  0.05405405405405406   \n",
       "Education                          740   1.2918918918918918   \n",
       "Son                                740    1.018918918918919   \n",
       "Social_drinker                     740   0.5675675675675675   \n",
       "Social_smoker                      740  0.07297297297297298   \n",
       "Pet                                740    0.745945945945946   \n",
       "Weight                             740    79.03513513513514   \n",
       "Height                             740   172.11486486486487   \n",
       "Body_mass_index                    740   26.677027027027027   \n",
       "Absenteeism_time_in_hours          740    6.924324324324324   \n",
       "\n",
       "                                                   2        3        4  \n",
       "summary                                       stddev      min      max  \n",
       "ID                                11.021247263063655        1       36  \n",
       "Reason_for_absence                 8.433405882799654        0       28  \n",
       "Month_of_absence                  3.4362869319125893        0       12  \n",
       "Day_of_the_week                   1.4216747097562803        2        6  \n",
       "Seasons                            1.111831060157382        1        4  \n",
       "Transportation_expense             66.95222324531973      118      388  \n",
       "Distance_from_Residence_to_Work   14.836788436739145        5       52  \n",
       "Service_time                       4.384873407621149        1       29  \n",
       "Age                                 6.47877245761187       27       58  \n",
       "Workload_per_day                   39.05811618814401  205.917  378.884  \n",
       "Hit_target                        3.7793131344179947       81      100  \n",
       "Disciplinary_failure             0.22627727323215055        0        1  \n",
       "Education                         0.6732380415251594        1        4  \n",
       "Son                               1.0984890195302819        0        4  \n",
       "Social_drinker                     0.495748667200035        0        1  \n",
       "Social_smoker                     0.2602680502800184        0        1  \n",
       "Pet                               1.3182582913258338        0        8  \n",
       "Weight                            12.883210507177221       56      108  \n",
       "Height                              6.03499453026766      163      196  \n",
       "Body_mass_index                    4.285452223167275       19       38  \n",
       "Absenteeism_time_in_hours         13.330998100978201        0      120  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absentee_data.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbff617-1068-4aba-8662-9ed44abfb90e",
   "metadata": {},
   "source": [
    "# Splitting the Data, Metrics, and Models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd7285-0763-426e-ac14-708d4fe73cf0",
   "metadata": {},
   "source": [
    "## Model Metrics  \n",
    "\n",
    "Model metrics are used to determine the quality of the predictions produced by our models. There are many different model metrics that are available to determine how close our predictions are to the actual values of the response variable, but the specific metric selected depends on the type of data you are attempting to make predictions for with your model.  \n",
    "\n",
    "In this example we are building models that will predict a continuous numeric variable (`Absenteeism time in hours`). Regression models are used when the goal is to predict a continuous response, and all of the models presented in this analysis will be regression models. The most common way to fit a regression model is based on minimizing the sum of squared errors, which can be done with or without including a penalty.  To evaluate the performance of our models we will utilize two of the most commonly used model metrics for regression problems, root mean squared error (RMSE) and mean absolute error (MAE).\n",
    "\n",
    "### RMSE - Root Mean Squared Error   \n",
    "\n",
    "RMSE is one of the most popular model metrics used to evaluate regression model performance. When comparing models built on the same target variable and the same dataset, the more accurate the model, the lower the RMSE value produced.  \n",
    "\n",
    "**RMSE Calculation**  \n",
    "$$\\Large\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "RMSE is the square root of the mean squared error between the predicted values produced by our model and the actual values of the target variable. This value is calculated by subtracting the predicted value from the actual value of our response variable and then squaring this value. This is then summed up over all of the observations in our dataset, divided by the number of observations, and then taking the square root of that resulting value.  \n",
    "\n",
    "One big advantage of using RMSE over other possible metrics is that it produces values that are in the same units as our response/target variable, and is therefore fairly easy to understand.   \n",
    "\n",
    "Potential disadvantages of using RMSE as a model metric are the fact that it can be more sensitive to outliers in the data than other methods and that it will penalize large errors in prediction more than other methods. This is due to the fact that in the RMSE calculation we are squaring the error, meaning that outliers will result in larger prediction errors and will be penalized more heavily than in other methods such as mean absolute error. It is important to note that this sensitivity to outliers can be a benefit depending on the context of the problem at hand. In many cases we may want to minimize occasional large mistakes in our predictions and would want to use a model metric that would penalize outliers more heavily. \n",
    "\n",
    "### MAE - Mean Absolute Error  \n",
    "\n",
    "**MAE Calculation**  \n",
    "$$\\Large|y_i - \\hat{y}_i|$$\n",
    "\n",
    "The mean absolute error, or MAE, is the average absolute value of the difference between our actual and predicted values for our response variable.  The main advantage of using this as a model metric is that MAE is easy to interpret as the value is on the same scale and in the same units as our response/target variable and is simply the average of the absolute error.  \n",
    "\n",
    "In contrast to RMSE, MAE does not heavily penalize large outliers. Depending on the situation, this may be appropriate, but it could also be a drawback if you are looking to penalize large outliers. If you want to reduce the impact of outliers in model evaluation, then MAE would be a more appropriate metric than RMSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e813f4-88d5-4bcc-b034-18f3c581f153",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training and Test Sets   \n",
    "\n",
    "The goal of creating our supervised models is to be able to successfully predict the number of absentee hours on new data, that is, on data that was not used to train our model. We want our model to be able to generalize to new data. In order to build a model that will be able to generalize we must be careful to not overfit our model to the data used to build it. If we build a model that is perfect at predicting our response variable based on data that was used to build the model, but largely fails to predict our response variable when new data is used to generate predictions, it is clear to see that our model will not be very useful. \n",
    "\n",
    "One common way to combat the problem of overfitting our model is to split the data into a training set and a test set. The exact split to use for a training vs test set can vary, but it is common to see an 80/20 or 70/30 training vs test set split. Once the data has been split into training and test sets we can use the training set to fit our model. Often we will want to use model metrics to evaluate the predictions produced by our model based on the observations contained in our training set that were used to fit that model.\n",
    "\n",
    "Now, let's split our data into training and test sets, named simply `train` and `test`.  We can do this by using the `.randomSplit()` method on our spark SQL data frame.  We will use an 80/20 split, sending 80% of the data to the training set, and 20% to the test set. We will print out the counts of each to ensure that the data has been split as expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7edbb19a-b50f-4199-a1ba-aa59329101bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581 159\n"
     ]
    }
   ],
   "source": [
    "train, test = absentee_data.randomSplit([0.8,0.2], seed = 1234)\n",
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38435012-34cf-4587-a581-3af73053367c",
   "metadata": {},
   "source": [
    "## Models  \n",
    "\n",
    "Statistical learning is used for inference, prediction, classification, and pattern finding based on your data. A statistical learning model is a mathematical representation of some phenomenon on which data has been observed and collected. We will build 5 different classes of supervised statistical learning models to predict the value of our response variable, the number of absentee hours. In this section we will briefly discuss the concepts and ideas involved in each of our five classes of models. In the next section, [Model Fitting Using Spark MLlib](#Model-Fitting-Using-Spark-MLlib), we will build and fit our models on the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c289e-b390-47f6-9292-97ccee1d5ba7",
   "metadata": {},
   "source": [
    "### Model 1  - Multiple Linear Regression  \n",
    "\n",
    "When there is a highly linear relationship between the predictor variables and the response, a linear regression model will work well and will likely outperform other types of models. MLR models are generally fit by minimizing the sum of squared residuals (errors), $y_i - \\hat{y}_i$. The general formula can be seen below:\n",
    "\n",
    "**Minimizing Sums of Squares without Penalty**  \n",
    "\n",
    "$$\\Large \\underset{\\hat{\\beta}'s}{\\min} \\sum_{i=1}^n (y_{i}-(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1i}+\\ldots+\\hat{\\beta}_{p}x_{pi}))^2$$\n",
    "\n",
    "This can be extended to include penalty terms as well. When a penalty term is included the model is referred to as a regularized regression or penalized regression model. An $L_1$ penalty ($\\alpha\\sum_{j=1}^p|\\beta_j|$) will shrink and do variable selection through Lasso regression. An $L_2$ penalty ($\\lambda \\sum_{j=1}^p \\beta_j ^2$) is useful for datasets with high collinearity among predictor variables and creates a Ridge regression model. Both penalties can also be combined in the elastic net model seen below:\n",
    "\n",
    "**Elastic Net Model Showing $L_1$ and $L_2$ Penalties**\n",
    "\n",
    "$$\\Large \\underset{\\beta's}{\\min} \\sum_{i=1}^n (y_{i}-(\\beta_{0}+\\beta_{1}x_{1i}+\\ldots+\\beta_{p}x_{pi}))^2+\\alpha\\sum_{j=1}^p|\\beta_j|+\\lambda \\sum_{j=1}^p \\beta_j ^2$$\n",
    "\n",
    "`LinearRegression` from `MLlib` can be used to create a standard linear model without penalty, as well as lasso, ridge regression, and elastic net models depending on the settings of the `regParam` and `elasticNetParam` parameters. The specifics of these will be discussed in the model fitting section.  For model 1 we will use cross validation and a set of tuning parameters to return the best linear regression/regularized regression model. Models under consideration will include standard multiple linear regression as lasso, ridge, and elastic net models.  \n",
    "\n",
    "The drawbacks of MLR models are that the response can take on any value, including negative numbers, which might not make sense for a given situation. In our data we will be predicting absentee hours, and it is easy to see how a negative prediction for this variable would not be meaningful. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6fbba-6430-4ef2-a878-40e25696286e",
   "metadata": {},
   "source": [
    "### Model 2  - Regression Tree  \n",
    "\n",
    "Regression trees are built on the idea of splitting the predictor space into regions, and then creating different predictions for each region. In this case we are building a regression tree as our goal is to predict a continuous numeric response, the number of absentee hours. Regression trees usually use the mean of observations within a region as the predicted value of the response variable. Regression trees are fit using recursive binary splitting. For every possible value of each predictor, the residual sum of squares is calculated, and the model fitting process tries to minimize this value at each split along the way.\n",
    "\n",
    "There are many benefits to using regression trees. Regression trees can be displayed graphically and are easily interpreted even by a non-expert. They can also handle qualitative predictors without the need to create dummy variables. Regression trees also have built-in variable selection, and automatically account for interactions between predictor variables, eliminating the need to specify interaction terms when creating the model. An interaction effect implies that the effect of one variable differs depending on the value of another variable. Regression trees split on more than one predictor variable, and as a result there is no need to include interaction terms as this is a fundamental part of the regression tree model building process. It is relatively easy to include categorical predictors, although they must be converted into dummy variables prior to building the model. Additionally, tree based models may outperform linear models when the predictor variables and response have a highly non-linear and complex relationship. \n",
    "\n",
    "There are a few big drawbacks when it comes to regression tree models. Regression trees tend to have high variance, meaning that even small changes in the data can vastly change the final tree produced. There is also no optimal algorithm for creating tree models, and as a result these models are built using a greedy algorithm at each split, which is only looking at the best option for the split that it is on at that time. This means that rather than considering what the best possible combination of splits would be, the greedy algorithm determines what the best possible split would be right now. Lastly, tree pruning is often needed to avoid overfitting. Overfitting our model on the training set will lead to decreased performance on the test set. A smaller tree with fewer splits will likely have a lower variance as well as better interpretability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171b7ca-10ce-47aa-afaa-465f09c18c34",
   "metadata": {},
   "source": [
    "### Model 3 - Random Forest  \n",
    "\n",
    "Random forests are ensemble models that help to reduce the variance of model predictions as compared to a single regression tree model. Random forest models are similar to bagged tree models in that they create multiple trees from bootstrap samples and average the results. The main difference is that random forests do not use all predictors. They instead use a random subset of the predictors for each bootstrap sample/tree fit. When building decision trees using this method each time a split in a tree is considered a random sample of $m$ predictors is chosen from the full set of $p$ predictors. The split is then allowed to use only one of the $m$ predictors. This avoids a problem that can arise if there is a single very strong predictor that exists in your data set. When this occurs every bootstrap tree is likely to use that predictor for its first split, resulting in bagged tree predictions that are more correlated to one another, and thereby giving a smaller reduction in the variance from aggregation. As the goal of ensemble methods is to reduce variance in order to increase prediction success, when there is one strong predictor we will likely want to select a random forest model over a bagged tree model. Additionally, if we have a data set containing a large number of correlated predictors we will see better results with a random forest model that uses a small value of $m$.\n",
    "\n",
    "Like other ensemble methods, random forests help to improve prediction accuracy at the expense of interpretability. One other drawback of this type of model is that it can be more costly to create, train, and deploy than other methods, like a single regression tree or MLR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16adb44-2378-483c-a036-8aac6ae8eb7c",
   "metadata": {},
   "source": [
    "### Model 4  - Gradient Boosted Regression Tree   \n",
    "\n",
    "A gradient boosted regression tree is another ensemble tree based method. In boosting the trees are grown successively, using a slow learning approach based on a loss function and weak learner. The algorithm will work to minimize the loss function. The algorithm begins with the best guess of the response. The gradient is calculated and the model is fit to minimize the loss function. The current model is created and the process continues for the specified iterations, modifying the model with each iteration. The final model is produced after all iterations have been completed.\n",
    "\n",
    "Boosting is similar to bagging in that many trees are created and then the trees are combined into a single predictive model. In bagging, each tree is built independent of other trees and trees are grown in parallel. In contrast, in a boosted tree model the trees are grown sequentially and each successive tree is built on information gained from the trees that came before it. Like other ensemble methods, boosted regression trees can result in a reduction in variance as compared to a single tree fit.\n",
    "\n",
    "There are several drawbacks to gradient boosted regression tree models. Boosted regression tree models are prone to overfitting with the addition of too many trees, resulting in decreased predictive performance on new data. Also, like other ensemble models, boosted trees can be computationally expensive and take a longer time to train compared to other methods.  \n",
    "\n",
    "NOTE: In `MLlib` the `GBTRegressor` builds trees successively but also allows the user to limit the number of features available at each split, as is done in the `RandomForestRegressor`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec3840-7113-478b-83a8-5fb121e1efbb",
   "metadata": {},
   "source": [
    "### Model 5 - LASSO Regression   \n",
    "Lasso stands for least angle subset and selection operator, and it is a type of regularized regression model. Fitting a model using lasso is similar to fitting a model using least squares, but a penalty is placed on the sum of the absolute values of the regression coefficients. Lasso regression relies on the tuning parameter $\\alpha$, which is a value $>0$. The formula can be seen below:  \n",
    "\n",
    "**LASSO Regression**  \n",
    "\n",
    "$$\\Large \\underset{\\beta's}{\\min} \\sum_{i=1}^n (y_{i}-(\\beta_{0}+\\beta_{1}x_{1i}+\\ldots+\\beta_{p}x_{pi}))^2+\\alpha\\sum_{j=1}^p|\\beta_j|$$\n",
    "\n",
    "Where:  \n",
    "$p = # predictors$  \n",
    "\n",
    "So, as the value of $\\alpha$ gets larger, it is penalizing having larger values of $\\beta$. As a result, it will start to shrink the $\\beta$s in our model. Often this results in some of the $\\beta$s being set to zero, which will drop these predictor variables out of the model.\n",
    "\n",
    "The main benefits of lasso regression are that it shrinks the coefficient estimates towards zero and does variable selection automatically. One big drawback of lasso regression is that it can be more difficult to interpret coefficients in the final model as they shrink towards zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007df2b-c9ef-4fb1-ac1a-08cabfff0ab7",
   "metadata": {},
   "source": [
    "# Model Fitting Using Spark MLlib and `CrossValidation()`\n",
    "\n",
    "We will be using cross-validation for model selection and hyperparameter tuning.  \n",
    "\n",
    "First, let's import the required libraries and functions that we will need to build our models. These will be used in all models, additional libraries or functions that are needed for specific models will be imported as needed within each model section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e3784ec0-5f21-46a0-b430-dd2952f25fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68891d8a-d119-4685-83ad-69fcf0ebd054",
   "metadata": {},
   "source": [
    "## Transformations and Preprocessing Using `MLlib` Functions  \n",
    "\n",
    "The transformations in this section will be used in one or more of our model pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8b746-2011-44bd-b144-423bc2462e6b",
   "metadata": {},
   "source": [
    "#### `sql_trans_label` Selecting Variables for Modeling and Creating Label Column  \n",
    "\n",
    "The `sql_trans_label` transformation uses `SQLTransformer()` to create the `label` column in our data set. We will also select only the variables that we want to include when building our models. We will drop the variables below from our dataset:\n",
    "\n",
    "-  `ID` - This is a personal identifier of each employee. As we are concerned with predicting the number of absentee hours for a given instance, `ID` will not be needed to build our model.  \n",
    "-  `Body_mass_index` - We will drop this variable as it is a function of height and weight, and is highly correlated with both.  \n",
    "-  `Month_of_absence` - We will drop this variable because it contains similar information to the `Seasons` variable, with many additional levels as compared to the `Seasons` variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f95241b-336e-4be2-8634-8e891b37f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_trans_label = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT Reason_for_absence, Day_of_the_week, Seasons, Transportation_expense, Distance_from_Residence_to_Work, Service_time, \\\n",
    "                Age, Workload_per_day, Hit_target, Disciplinary_failure, Education, Son, Social_drinker, Social_smoker, Pet, Weight, \\\n",
    "                Height, Absenteeism_time_in_hours AS label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2907d8c-acf3-455c-9271-b6260788c17f",
   "metadata": {},
   "source": [
    "Let's just do a quick check that this works as expected. We will use `.toPandas()` to display the data frame as a pandas dataframe. This will not change the `absentee_data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7046c3af-30f7-4052-b366-4429d05f36e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reason_for_absence',\n",
       " 'Day_of_the_week',\n",
       " 'Seasons',\n",
       " 'Transportation_expense',\n",
       " 'Distance_from_Residence_to_Work',\n",
       " 'Service_time',\n",
       " 'Age',\n",
       " 'Workload_per_day',\n",
       " 'Hit_target',\n",
       " 'Disciplinary_failure',\n",
       " 'Education',\n",
       " 'Son',\n",
       " 'Social_drinker',\n",
       " 'Social_smoker',\n",
       " 'Pet',\n",
       " 'Weight',\n",
       " 'Height',\n",
       " 'label']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_trans_label.transform(absentee_data).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c46e6-3345-4191-94f0-a8d9fd52ab2d",
   "metadata": {},
   "source": [
    "#### `scale_trans` Standardizing Numeric Variables  \n",
    "\n",
    "It is important that we standardize our numeric variables before building our models. Some of the predictor variables that we are using have different units and very different scales, such as `Transporation_expense` with a maximum value of `388` compared to `Son` (number of children) with a maximum value of `4`.  The variables the we will standardize are:  \n",
    "\n",
    "-  `Transportation_expense` - The standardized version will be called `Transportaion_expense_scaled`  \n",
    "-  `Distance_from_Residence_to_Work` The standardized version will be called `Distance_from_Residence_to_Work_scaled`  \n",
    "-  `Service_time` The standardized version will be called `Service_time_scaled`  \n",
    "-  `Age` The standardized version will be called `Age_scaled`  \n",
    "-  `Workload_per_day` The standardized version will be called `Workload_per_day_scaled`\n",
    "-  `Hit_target` The standardized version will be called `Hit_target_scaled`  \n",
    "-  `Son` The standardized version will be called `Son_scaled`  \n",
    "-  `Pet` The standardized version will be called `Pet_scaled`  \n",
    "-  `Weight` The standardized version will be called `Weight_scaled`  \n",
    "-  `Height` The standardized version will be called `Height_scaled`  \n",
    "\n",
    "*Note that we will add these to our data frame without removing the original variables*  \n",
    "\n",
    "There are multiple steps involved here, as annotated in the comments within the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "dcad7fba-9b82-4834-89b0-59589b2e53ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason_for_absence</th>\n",
       "      <th>Month_of_absence</th>\n",
       "      <th>Day_of_the_week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation_expense</th>\n",
       "      <th>Distance_from_Residence_to_Work</th>\n",
       "      <th>Service_time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Workload_per_day</th>\n",
       "      <th>...</th>\n",
       "      <th>Transportation_expense_scaled</th>\n",
       "      <th>Distance_from_Residence_to_Work_scaled</th>\n",
       "      <th>Service_time_scaled</th>\n",
       "      <th>Age_scaled</th>\n",
       "      <th>Workload_per_day_scaled</th>\n",
       "      <th>Hit_target_scaled</th>\n",
       "      <th>Son_scaled</th>\n",
       "      <th>Pet_scaled</th>\n",
       "      <th>Weight_scaled</th>\n",
       "      <th>Height_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>37</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.1819759583127146]</td>\n",
       "      <td>[-1.2713234417741466]</td>\n",
       "      <td>[0.30863024031557357]</td>\n",
       "      <td>[0.07337951184821305]</td>\n",
       "      <td>[-0.7909860178457714]</td>\n",
       "      <td>[0.6387630526509107]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[0.1507529537531695]</td>\n",
       "      <td>[0.6567778313315372]</td>\n",
       "      <td>[0.0008946321330850656]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>205.917</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.1819759583127146]</td>\n",
       "      <td>[-0.05845165249536284]</td>\n",
       "      <td>[-0.14190787431595467]</td>\n",
       "      <td>[1.7542700502780677]</td>\n",
       "      <td>[-1.6225070263874686]</td>\n",
       "      <td>[-0.6689231122193513]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[3.0948694623444797]</td>\n",
       "      <td>[0.6567778313315372]</td>\n",
       "      <td>[-1.5584491758310226]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>205.917</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.1819759583127146]</td>\n",
       "      <td>[-0.05845165249536284]</td>\n",
       "      <td>[-0.14190787431595467]</td>\n",
       "      <td>[1.7542700502780677]</td>\n",
       "      <td>[-1.6225070263874686]</td>\n",
       "      <td>[-0.6689231122193513]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[3.0948694623444797]</td>\n",
       "      <td>[0.6567778313315372]</td>\n",
       "      <td>[-1.5584491758310226]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.6502941822077359]</td>\n",
       "      <td>[1.423947201067595]</td>\n",
       "      <td>[1.20970646957863]</td>\n",
       "      <td>[0.22618774261456345]</td>\n",
       "      <td>[-0.7909860178457714]</td>\n",
       "      <td>[0.6387630526509107]</td>\n",
       "      <td>[-0.9371782960212637]</td>\n",
       "      <td>[-0.5852761733946581]</td>\n",
       "      <td>[0.7345418256261415]</td>\n",
       "      <td>[-0.3456262140811611]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>205.917</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.6502941822077359]</td>\n",
       "      <td>[1.423947201067595]</td>\n",
       "      <td>[1.20970646957863]</td>\n",
       "      <td>[0.22618774261456345]</td>\n",
       "      <td>[-1.6225070263874686]</td>\n",
       "      <td>[-0.6689231122193513]</td>\n",
       "      <td>[-0.9371782960212637]</td>\n",
       "      <td>[-0.5852761733946581]</td>\n",
       "      <td>[0.7345418256261415]</td>\n",
       "      <td>[-0.3456262140811611]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>246.288</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.5568741567032267]</td>\n",
       "      <td>[-1.1365599096320596]</td>\n",
       "      <td>[1.20970646957863]</td>\n",
       "      <td>[2.0598865118107685]</td>\n",
       "      <td>[-0.6245186613181442]</td>\n",
       "      <td>[-0.9304603451934037]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[-0.5852761733946581]</td>\n",
       "      <td>[1.4344177742775797]</td>\n",
       "      <td>[1.0404571707758237]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>275.089</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.5568741567032267]</td>\n",
       "      <td>[-1.1365599096320596]</td>\n",
       "      <td>[1.20970646957863]</td>\n",
       "      <td>[2.0598865118107685]</td>\n",
       "      <td>[0.08745436145467776]</td>\n",
       "      <td>[0.37722581967685836]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[-0.5852761733946581]</td>\n",
       "      <td>[1.4344177742775797]</td>\n",
       "      <td>[1.0404571707758237]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>275.089</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.5568741567032267]</td>\n",
       "      <td>[-1.1365599096320596]</td>\n",
       "      <td>[1.20970646957863]</td>\n",
       "      <td>[2.0598865118107685]</td>\n",
       "      <td>[0.08745436145467776]</td>\n",
       "      <td>[0.37722581967685836]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[-0.5852761733946581]</td>\n",
       "      <td>[1.4344177742775797]</td>\n",
       "      <td>[1.0404571707758237]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>246.288</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.5568741567032267]</td>\n",
       "      <td>[-1.1365599096320596]</td>\n",
       "      <td>[1.20970646957863]</td>\n",
       "      <td>[2.0598865118107685]</td>\n",
       "      <td>[-0.6245186613181442]</td>\n",
       "      <td>[-0.9304603451934037]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[-0.5852761733946581]</td>\n",
       "      <td>[1.4344177742775797]</td>\n",
       "      <td>[1.0404571707758237]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>237.656</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.5568741567032267]</td>\n",
       "      <td>[-1.1365599096320596]</td>\n",
       "      <td>[1.20970646957863]</td>\n",
       "      <td>[2.0598865118107685]</td>\n",
       "      <td>[-0.8379053886045232]</td>\n",
       "      <td>[1.1618375185990155]</td>\n",
       "      <td>[0.016412929877780518]</td>\n",
       "      <td>[-0.5852761733946581]</td>\n",
       "      <td>[1.4344177742775797]</td>\n",
       "      <td>[1.0404571707758237]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Reason_for_absence  Month_of_absence  Day_of_the_week  Seasons  \\\n",
       "0     1                  22                 7                2        1   \n",
       "1     2                  18                 8                2        1   \n",
       "2     2                  18                 8                5        1   \n",
       "3     3                  11                 7                4        1   \n",
       "4     3                  11                 8                2        1   \n",
       "..   ..                 ...               ...              ...      ...   \n",
       "576  36                  14                 4                5        3   \n",
       "577  36                  19                 6                5        3   \n",
       "578  36                  19                 6                5        3   \n",
       "579  36                  23                 4                3        3   \n",
       "580  36                  23                 5                2        3   \n",
       "\n",
       "     Transportation_expense  Distance_from_Residence_to_Work  Service_time  \\\n",
       "0                       235                               11            14   \n",
       "1                       235                               29            12   \n",
       "2                       235                               29            12   \n",
       "3                       179                               51            18   \n",
       "4                       179                               51            18   \n",
       "..                      ...                              ...           ...   \n",
       "576                     118                               13            18   \n",
       "577                     118                               13            18   \n",
       "578                     118                               13            18   \n",
       "579                     118                               13            18   \n",
       "580                     118                               13            18   \n",
       "\n",
       "     Age  Workload_per_day  ...  Transportation_expense_scaled  \\\n",
       "0     37           239.554  ...           [0.1819759583127146]   \n",
       "1     48           205.917  ...           [0.1819759583127146]   \n",
       "2     48           205.917  ...           [0.1819759583127146]   \n",
       "3     38           239.554  ...          [-0.6502941822077359]   \n",
       "4     38           205.917  ...          [-0.6502941822077359]   \n",
       "..   ...               ...  ...                            ...   \n",
       "576   50           246.288  ...          [-1.5568741567032267]   \n",
       "577   50           275.089  ...          [-1.5568741567032267]   \n",
       "578   50           275.089  ...          [-1.5568741567032267]   \n",
       "579   50           246.288  ...          [-1.5568741567032267]   \n",
       "580   50           237.656  ...          [-1.5568741567032267]   \n",
       "\n",
       "     Distance_from_Residence_to_Work_scaled     Service_time_scaled  \\\n",
       "0                     [-1.2713234417741466]   [0.30863024031557357]   \n",
       "1                    [-0.05845165249536284]  [-0.14190787431595467]   \n",
       "2                    [-0.05845165249536284]  [-0.14190787431595467]   \n",
       "3                       [1.423947201067595]      [1.20970646957863]   \n",
       "4                       [1.423947201067595]      [1.20970646957863]   \n",
       "..                                      ...                     ...   \n",
       "576                   [-1.1365599096320596]      [1.20970646957863]   \n",
       "577                   [-1.1365599096320596]      [1.20970646957863]   \n",
       "578                   [-1.1365599096320596]      [1.20970646957863]   \n",
       "579                   [-1.1365599096320596]      [1.20970646957863]   \n",
       "580                   [-1.1365599096320596]      [1.20970646957863]   \n",
       "\n",
       "                Age_scaled  Workload_per_day_scaled      Hit_target_scaled  \\\n",
       "0    [0.07337951184821305]    [-0.7909860178457714]   [0.6387630526509107]   \n",
       "1     [1.7542700502780677]    [-1.6225070263874686]  [-0.6689231122193513]   \n",
       "2     [1.7542700502780677]    [-1.6225070263874686]  [-0.6689231122193513]   \n",
       "3    [0.22618774261456345]    [-0.7909860178457714]   [0.6387630526509107]   \n",
       "4    [0.22618774261456345]    [-1.6225070263874686]  [-0.6689231122193513]   \n",
       "..                     ...                      ...                    ...   \n",
       "576   [2.0598865118107685]    [-0.6245186613181442]  [-0.9304603451934037]   \n",
       "577   [2.0598865118107685]    [0.08745436145467776]  [0.37722581967685836]   \n",
       "578   [2.0598865118107685]    [0.08745436145467776]  [0.37722581967685836]   \n",
       "579   [2.0598865118107685]    [-0.6245186613181442]  [-0.9304603451934037]   \n",
       "580   [2.0598865118107685]    [-0.8379053886045232]   [1.1618375185990155]   \n",
       "\n",
       "                 Son_scaled             Pet_scaled         Weight_scaled  \\\n",
       "0    [0.016412929877780518]   [0.1507529537531695]  [0.6567778313315372]   \n",
       "1    [0.016412929877780518]   [3.0948694623444797]  [0.6567778313315372]   \n",
       "2    [0.016412929877780518]   [3.0948694623444797]  [0.6567778313315372]   \n",
       "3     [-0.9371782960212637]  [-0.5852761733946581]  [0.7345418256261415]   \n",
       "4     [-0.9371782960212637]  [-0.5852761733946581]  [0.7345418256261415]   \n",
       "..                      ...                    ...                   ...   \n",
       "576  [0.016412929877780518]  [-0.5852761733946581]  [1.4344177742775797]   \n",
       "577  [0.016412929877780518]  [-0.5852761733946581]  [1.4344177742775797]   \n",
       "578  [0.016412929877780518]  [-0.5852761733946581]  [1.4344177742775797]   \n",
       "579  [0.016412929877780518]  [-0.5852761733946581]  [1.4344177742775797]   \n",
       "580  [0.016412929877780518]  [-0.5852761733946581]  [1.4344177742775797]   \n",
       "\n",
       "               Height_scaled  \n",
       "0    [0.0008946321330850656]  \n",
       "1      [-1.5584491758310226]  \n",
       "2      [-1.5584491758310226]  \n",
       "3      [-0.3456262140811611]  \n",
       "4      [-0.3456262140811611]  \n",
       "..                       ...  \n",
       "576     [1.0404571707758237]  \n",
       "577     [1.0404571707758237]  \n",
       "578     [1.0404571707758237]  \n",
       "579     [1.0404571707758237]  \n",
       "580     [1.0404571707758237]  \n",
       "\n",
       "[581 rows x 41 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of numeric column names that we want to scale\n",
    "numeric_cols = ['Transportation_expense', 'Distance_from_Residence_to_Work', 'Service_time', 'Age', 'Workload_per_day', \\\n",
    "                'Hit_target', 'Son', 'Pet', 'Weight', 'Height']\n",
    "\n",
    "# Creating a vector of values for each of our variables above\n",
    "assembler = [VectorAssembler(inputCols=[col], outputCol=col+'_vec') for col in numeric_cols]\n",
    "\n",
    "# Using StandardScaler to create scaled versions of our numeric variables\n",
    "scale = [StandardScaler(inputCol=col+'_vec', outputCol=col+'_scaled', withMean=True, withStd=True) for col in numeric_cols]\n",
    "\n",
    "# Putting everything together in a pipeline that can be used to scale our data\n",
    "scale_pipe = Pipeline(stages = assembler + scale)\n",
    "\n",
    "# Fitting the pipeline on our training data\n",
    "scale_trans = scale_pipe.fit(train)\n",
    "\n",
    "# Check how our scaled variables in our train set look\n",
    "scale_trans.transform(train).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4f0a4-e182-40fe-856e-6d0e6dee3521",
   "metadata": {},
   "source": [
    "We will now be able to use `scale_trans` in our model pipeline to scale based on our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186aecd1-66e4-44f8-aee9-c7b4112c9fcd",
   "metadata": {},
   "source": [
    "#### `encoder_trans` Creating Dummy Variables for Categorical Variables in the Dataset  \n",
    "\n",
    "Before we build our models we want to create dummy variables for the non-binary categorical variables in our dataset. We will use `OneHotEncoder()` from `MLlib` to create these dummy variables for us. We will create dummy variables for:  \n",
    "-  `Reason_for_absence` - The encoded reason for absence variable will be called `Reason_onehot`    \n",
    "-  `Day_of_the_week` - The encoded day of the week variable will be called `Day_onehot`  \n",
    "-  `Seasons` - The encoded seasons variable will be called `Season_onehot`  \n",
    "\n",
    "*Note that we will add these to our data frame without removing the original variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "de1a9c92-a40a-4a3b-9edd-f90d52ec77c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create our encoder instance and save as onehot_encoder\n",
    "onehot_encoder = OneHotEncoder(inputCols=['Reason_for_absence', 'Day_of_the_week', 'Seasons'],\n",
    "                               outputCols=['Reason_onehot', 'Day_onehot', 'Season_onehot'])\n",
    "\n",
    "# Fit our encoder instance on our train data frame and save it as encoder_trans to be used in our pipeline\n",
    "encoder_trans = onehot_encoder.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16aec6-0bf7-4bcb-9336-ccdbd3b4bc6b",
   "metadata": {},
   "source": [
    "Let's check to make sure that this works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "da64e219-be21-4e71-91cf-f48e5baa68bc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+---------------+-------------+-------------+\n",
      "| ID|Reason_for_absence|Month_of_absence|Day_of_the_week|Seasons|Transportation_expense|Distance_from_Residence_to_Work|Service_time|Age|Workload_per_day|Hit_target|Disciplinary_failure|Education|Son|Social_drinker|Social_smoker|Pet|Weight|Height|Body_mass_index|Absenteeism_time_in_hours|  Reason_onehot|   Day_onehot|Season_onehot|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+---------------+-------------+-------------+\n",
      "|  1|                22|               7|              2|      1|                   235|                             11|          14| 37|         239.554|        97|                   0|        3|  1|             0|            0|  1|    88|   172|             29|                        8|(28,[22],[1.0])|(6,[2],[1.0])|(4,[1],[1.0])|\n",
      "|  2|                18|               8|              2|      1|                   235|                             29|          12| 48|         205.917|        92|                   0|        1|  1|             0|            1|  5|    88|   163|             33|                        8|(28,[18],[1.0])|(6,[2],[1.0])|(4,[1],[1.0])|\n",
      "|  2|                18|               8|              5|      1|                   235|                             29|          12| 48|         205.917|        92|                   0|        1|  1|             0|            1|  5|    88|   163|             33|                        8|(28,[18],[1.0])|(6,[5],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                11|               7|              4|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        1|(28,[11],[1.0])|(6,[4],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                11|               8|              2|      1|                   179|                             51|          18| 38|         205.917|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        1|(28,[11],[1.0])|(6,[2],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                13|              11|              5|      4|                   179|                             51|          18| 38|         306.345|        93|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        8|(28,[13],[1.0])|(6,[5],[1.0])|    (4,[],[])|\n",
      "|  3|                18|               8|              2|      1|                   179|                             51|          18| 38|         205.917|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        8|(28,[18],[1.0])|(6,[2],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                21|               7|              2|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        8|(28,[21],[1.0])|(6,[2],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                23|               7|              4|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|(28,[23],[1.0])|(6,[4],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                23|               7|              4|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        4|(28,[23],[1.0])|(6,[4],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                23|               7|              6|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|(28,[23],[1.0])|    (6,[],[])|(4,[1],[1.0])|\n",
      "|  3|                23|               7|              6|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|(28,[23],[1.0])|    (6,[],[])|(4,[1],[1.0])|\n",
      "|  3|                23|               8|              2|      1|                   179|                             51|          18| 38|         205.917|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|(28,[23],[1.0])|(6,[2],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                23|               8|              6|      1|                   179|                             51|          18| 38|         205.917|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|(28,[23],[1.0])|    (6,[],[])|(4,[1],[1.0])|\n",
      "|  3|                23|               9|              3|      1|                   179|                             51|          18| 38|         241.476|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        3|(28,[23],[1.0])|(6,[3],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                23|               9|              3|      1|                   179|                             51|          18| 38|         241.476|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        4|(28,[23],[1.0])|(6,[3],[1.0])|(4,[1],[1.0])|\n",
      "|  3|                23|               9|              4|      4|                   179|                             51|          18| 38|         241.476|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        3|(28,[23],[1.0])|(6,[4],[1.0])|    (4,[],[])|\n",
      "|  3|                23|               9|              6|      4|                   179|                             51|          18| 38|         241.476|        92|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        3|(28,[23],[1.0])|    (6,[],[])|    (4,[],[])|\n",
      "|  3|                23|              10|              3|      4|                   179|                             51|          18| 38|         253.465|        93|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        3|(28,[23],[1.0])|(6,[3],[1.0])|    (4,[],[])|\n",
      "|  3|                23|              10|              6|      4|                   179|                             51|          18| 38|         253.465|        93|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        3|(28,[23],[1.0])|    (6,[],[])|    (4,[],[])|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+---------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test encoder_model by transforming our train set and save as encoded\n",
    "encoded = encoder_trans.transform(train)\n",
    "\n",
    "# Check that everything looks as expected using the action .show()\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06d2af-fd94-4cc4-82cf-102c783c7dec",
   "metadata": {},
   "source": [
    "#### Vector Assemblers  \n",
    "\n",
    "##### `features_Assembler`  \n",
    "\n",
    "The `vectorAssembler` will create a features vector containing all predictor variables that we want to use in our linear models that will use scaled data in addition to the one hot encoded variables and a couple of unchanged variables from our dataset. We will call this vector assembler `features_Assembler`. We need to pass these variable names as a list. To easily grab the columns we want we can use the `.columns` attribute on our spark SQL data frames to return a list of column names. We can then copy and paste the variables we want to include in our `features` column. We will include the variables: \n",
    "\n",
    "`Transportation_expense_scaled`, `Distance_from_Residence_to_Work_scaled`, `Service_time_scaled`, `Age_scaled`, `Workload_per_day_scaled`, `Hit_target_scaled`, `Son_scaled`, `Pet_scaled`, `Weight_scaled`, `Height_scaled`, `Reason_onehot`, `Day_onehot`, `Season_onehot`, `Disciplinary_failure`, `Education`, `Social_drinker`, and`Social_smoker`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f7db2d5f-a6b6-42a5-99ec-d9f04b4e7405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Reason_for_absence', 'Month_of_absence', 'Day_of_the_week', 'Seasons', 'Transportation_expense', 'Distance_from_Residence_to_Work', 'Service_time', 'Age', 'Workload_per_day', 'Hit_target', 'Disciplinary_failure', 'Education', 'Son', 'Social_drinker', 'Social_smoker', 'Pet', 'Weight', 'Height', 'Body_mass_index', 'Absenteeism_time_in_hours', 'Transportation_expense_vec', 'Distance_from_Residence_to_Work_vec', 'Service_time_vec', 'Age_vec', 'Workload_per_day_vec', 'Hit_target_vec', 'Son_vec', 'Pet_vec', 'Weight_vec', 'Height_vec', 'Transportation_expense_scaled', 'Distance_from_Residence_to_Work_scaled', 'Service_time_scaled', 'Age_scaled', 'Workload_per_day_scaled', 'Hit_target_scaled', 'Son_scaled', 'Pet_scaled', 'Weight_scaled', 'Height_scaled']\n"
     ]
    }
   ],
   "source": [
    "print(scale_trans.transform(absentee_data).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3512365b-c9f5-4f75-a2fb-1a3ec42506e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Reason_for_absence', 'Month_of_absence', 'Day_of_the_week', 'Seasons', 'Transportation_expense', 'Distance_from_Residence_to_Work', 'Service_time', 'Age', 'Workload_per_day', 'Hit_target', 'Disciplinary_failure', 'Education', 'Son', 'Social_drinker', 'Social_smoker', 'Pet', 'Weight', 'Height', 'Body_mass_index', 'Absenteeism_time_in_hours', 'Reason_onehot', 'Day_onehot', 'Season_onehot']\n"
     ]
    }
   ],
   "source": [
    "print(encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "431a7335-96f9-4c4c-98d9-dd34e9a30632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Reason_for_absence',\n",
       " 'Month_of_absence',\n",
       " 'Day_of_the_week',\n",
       " 'Seasons',\n",
       " 'Transportation_expense',\n",
       " 'Distance_from_Residence_to_Work',\n",
       " 'Service_time',\n",
       " 'Age',\n",
       " 'Workload_per_day',\n",
       " 'Hit_target',\n",
       " 'Disciplinary_failure',\n",
       " 'Education',\n",
       " 'Son',\n",
       " 'Social_drinker',\n",
       " 'Social_smoker',\n",
       " 'Pet',\n",
       " 'Weight',\n",
       " 'Height',\n",
       " 'Body_mass_index',\n",
       " 'Absenteeism_time_in_hours']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absentee_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "af303518-9b1f-4fac-953d-2f309d45f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Assembler = VectorAssembler(inputCols = ['Transportation_expense_scaled', 'Distance_from_Residence_to_Work_scaled', 'Service_time_scaled', \\\n",
    "                                                        'Age_scaled', 'Workload_per_day_scaled', 'Hit_target_scaled', 'Son_scaled', 'Pet_scaled', \\\n",
    "                                                        'Weight_scaled', 'Height_scaled', 'Reason_onehot', 'Day_onehot', 'Season_onehot',  \\\n",
    "                                                        'Disciplinary_failure', 'Education', 'Social_drinker', 'Social_smoker'], \n",
    "                                  outputCol = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20579ef9-2dbd-4bdd-b3fa-a6372ba79297",
   "metadata": {},
   "source": [
    "##### `tree_features_Assembler`  \n",
    "\n",
    "We will create a separate vector assembler to use with our tree models called `tree_features_assembler` and include the factors: \n",
    "\n",
    "`Transportation_expense`, `Distance_from_Residence_to_Work`, `Service_time`, `Age`, `Workload_per_day`, `Hit_target`, `Disciplinary_failure`, `Education`, `Son`, `Social_drinker`, `Social_smoker`, `Pet`, `Weight`, `Height`, `Reason_onehot`, `Day_onehot`, `Season_onehot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8d435765-6ee3-44fe-8916-7240a86afe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_features_Assembler = VectorAssembler(inputCols = ['Reason_onehot', 'Day_onehot', 'Season_onehot', 'Transportation_expense', \\\n",
    "                                                       'Distance_from_Residence_to_Work', 'Service_time', 'Age', 'Workload_per_day', \\\n",
    "                                                       'Hit_target', 'Disciplinary_failure', 'Education', 'Son', 'Social_drinker', \\\n",
    "                                                       'Social_smoker', 'Pet', 'Weight', 'Height'], \n",
    "                                  outputCol = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cc91c-a725-4ecf-9c84-e325c4165f20",
   "metadata": {},
   "source": [
    "## Cross-Validation  \n",
    "\n",
    "We will be using 5 fold cross-validation for model selection and hyperparameter tuning. In 5 fold cross-validation the data is split into 5 folds, or groups. Then, we train the model on the first 4 folds, test on the 5th fold, and find the sum of the loss function that we are using (like RMSE). We then move on and train our model on folds 1, 2, 3 and 5 and test on the 4th fold, again finding the sum of the loss function for this round. We continue until each of our folds has been used as the test set. Then we can find the CV error by summing all the test errors from above. The main idea for k fold cross-validation is that for each round we essentially have a mini train vs test set, where the loss function is based on the results from the “test” set that was not used to train the model for that round.\n",
    "\n",
    "We will use `CrossValidator()` from `pyspark.ml.tuning` to select the optimal parameter values for our models. `CrossValidator()` only allows for a single `evaluator` at a time. As we want to select optimal parameters based on RMSE and MAE we will have to fit two cross-validators per model type and compare the results. For each of our model classes, we will evaluate optimal parameters first based on RMSE, and then based on MAE. If the two metrics disagree on the model type to select for a given class of model more discussion or investigation may be needed to determine which model should be used to move on to predictions on our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b95eb-23ee-413d-9c95-e362ecab7af3",
   "metadata": {},
   "source": [
    "### Model 1  - Multiple Linear Regression  \n",
    "\n",
    "We will use `LinearRegression()` from `pyspark.ml.regression` for our MLR model. In linear regression, adding a penalty term to the loss function is called penalized regression or regularized regression. We will evaluate models for a range of possible penalty terms including least squares estimates without penalty, with an $L_1$ ($\\alpha$) penalty (LASSO), with an $L_2$ ($\\lambda$) penalty (Ridge Regression), and with a combination of both $L_1$ and $L_2$ penalty types (Elastic Net).\n",
    "\n",
    "The parameters available in `LinearRegression()` allow for various types of regularization. The `regParam` represents the $\\lambda$ in the penalty part of the loss function. If $\\lambda$ is set to `0` the penalty term $L_2$ will have no effect.   \n",
    "\n",
    "The `elasticNetParam` is the elastic net mixing parameter and has a range of 0 to 1. If this parameter is set to `1`, the resulting model is equivalent to a Lasso model with an $L_1$ penalty. If the `elasticNetParam` is set to `0`, the resulting model produced is a ridge regression model with an $L_2$ penalty. If both `regParam` and `elasticNetParam` are set to `0`, the resulting model will not contain a penalty term, and will be created based on the least squares estimates only.\n",
    "\n",
    "The series of transformations that we will use in our MLR pipeline are:  \n",
    "\n",
    "1. `sql_trans_label` - To select the columns of interest from our original data frame and to create our `label` column for our target variable, `Absenteeism_time_in_hours`.  \n",
    "2.  `scale_trans` - To standardize our numeric columns.  \n",
    "3.  `encoder_trans` To create one hot encoded versions of our categorical variables.  \n",
    "4.  `features_Assembler` To put all of our desired predictor variables into our `features` vector.  \n",
    "5.  `mlr_regressor` To create our MLR model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1b583a7b-29ea-4626-bb22-933ec6793dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LinearRegression\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bf0b71da-a9b8-49a6-bede-2f2e8afda0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mlr regressor instance for pipeline\n",
    "mlr_regressor = LinearRegression(featuresCol = 'features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "39b4be7c-e4a3-4aa0-96b1-9db467a17f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mlr pipeline \n",
    "mlr_pipeline = Pipeline(stages = [sql_trans_label, scale_trans, encoder_trans, features_Assembler, mlr_regressor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d809c3-0b68-4f3a-9966-088e9ff47cbc",
   "metadata": {},
   "source": [
    "### Cross-Validation and Hyperparameter Tuning   \n",
    "\n",
    "We can use `ParamGridBuilder()` to create a grid of all possible values of our parameters that we would like to consider for our model. Here we will select five values for `regParam` and five values for `elasticNetParam`. By including a possible value of `0` for both parameters as well as a possible value of `1` for the `elasticNetParam` we are able to include a non-penalized MLR model, a Lasso model, a Ridge regression model, and an elastic net model as possible models to be chosen in cross-validation in combination with the `regParam` values. Note that each additional value included for a parameter when building the param grid will increase the computational time during cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e4286273-2cef-4257-a0a4-b172f1374806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mlr param grid for cross validation\n",
    "mlr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(mlr_regressor.regParam, [0, 0.01, 0.1, 1.0, 10.0]) \\\n",
    "    .addGrid(mlr_regressor.elasticNetParam, [0.0, 0.2, 0.5, 0.7, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301704aa-72e4-447a-943d-cb6a5e640cf0",
   "metadata": {},
   "source": [
    "#### Cross Validation Using RMSE  \n",
    "\n",
    "Next, we will create our MLR crossvalidator, `mlr_crossval`, using the `mlr_pipeline`, `mlr_paramgrid`, and using RMSE as our metric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3aeb43ab-de3c-4871-b4ed-5abf15ff6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mlr crossvalidator using the mlr_pipleline, mlr_paramgrid, and using RMSE as our metric\n",
    "mlr_crossval = CrossValidator(estimator = mlr_pipeline,\n",
    "                              estimatorParamMaps = mlr_paramGrid,\n",
    "                              evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                              numFolds=5,\n",
    "                              seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5d44b-8681-465d-8e69-6a504386aba5",
   "metadata": {},
   "source": [
    "We can now fit our model on our `train` data set using `mlr_crossval`. This will automatically select the optimal values for our `regParam` and `elasticNetParam` parameters. We will save this as `mlr_cv_model`. This will allow us to then see the parameters selected by cross validation using RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2be6c091-6987-4511-8fec-689b03b2b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the mlr cross validator that we created\n",
    "mlr_cv_model = mlr_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "47698378-f531-4386-822a-c53f32407400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 10.0)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.0)\n",
      "[0.22677380411425374,-0.6254652539348092,-0.0159725373198707,0.24793320679231476,-0.15002026669437632,0.32419134027951324,0.8118695890234615,-0.33417633062839713,-0.06849474448941958,0.8702697889471593,-2.657495662444544,3.71094588203646,9.655008294267665,0.716702542210664,0.26688118102923386,-1.438637790992719,-0.08419063550457198,3.5350050397497665,-1.121249118850272,20.831941780091537,2.402572196174129,2.779815412863692,10.305868007015134,5.760164243153687,2.3388091160929783,-1.109096253060802,-3.186659236689779,1.4147473249335898,0.3175929729162821,8.204818826352563,0.0,-0.25314399328820825,0.67077058535416,-1.947262730852867,0.15712024147774584,-1.81265781401036,-0.08791012029731454,-1.3312181407370605,0.0,0.0,1.3110492718409235,0.2977772889965755,0.5512748130668081,-1.251882108722081,0.0,0.10957800910752284,-0.8707208703712623,0.37989140631742485,-2.408703410980144,-0.7089563385307599,0.709976295632011,0.028024955052427285]\n"
     ]
    }
   ],
   "source": [
    "# MLR model stats\n",
    "best_mlr_Pipeline = mlr_cv_model.bestModel\n",
    "best_mlr_Model = best_mlr_Pipeline.stages[4]\n",
    "print(best_mlr_Model.explainParam('regParam'))\n",
    "print(best_mlr_Model.explainParam('elasticNetParam'))\n",
    "print(best_mlr_Model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28969cd2-e45a-4d64-8455-879debda2e12",
   "metadata": {},
   "source": [
    "#### Cross Validation Using MAE  \n",
    "\n",
    "Next, we will run cross validation using MAE as our metric and see if both cross validators return the same model, or if different models were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "90091319-4d03-4cfe-ae85-9dedf68ef593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mlr mae crossvalidator using the mlr_pipleline, mlr_paramgrid, and using RMSE as our metric\n",
    "mlr_mae_crossval = CrossValidator(estimator = mlr_pipeline,\n",
    "                                  estimatorParamMaps = mlr_paramGrid,\n",
    "                                  evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                                  numFolds=5,\n",
    "                                  seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "872e4f5d-22e9-474e-a0cf-41f31304f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the mlr mae cross validator that we created\n",
    "mlr_mae_cv_model = mlr_mae_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e2bb63ac-4dfc-4088-8b5b-162612c0bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.0)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 10.0)\n",
      "[0.22677380411425374,-0.6254652539348092,-0.0159725373198707,0.24793320679231476,-0.15002026669437632,0.32419134027951324,0.8118695890234615,-0.33417633062839713,-0.06849474448941958,0.8702697889471593,-2.657495662444544,3.71094588203646,9.655008294267665,0.716702542210664,0.26688118102923386,-1.438637790992719,-0.08419063550457198,3.5350050397497665,-1.121249118850272,20.831941780091537,2.402572196174129,2.779815412863692,10.305868007015134,5.760164243153687,2.3388091160929783,-1.109096253060802,-3.186659236689779,1.4147473249335898,0.3175929729162821,8.204818826352563,0.0,-0.25314399328820825,0.67077058535416,-1.947262730852867,0.15712024147774584,-1.81265781401036,-0.08791012029731454,-1.3312181407370605,0.0,0.0,1.3110492718409235,0.2977772889965755,0.5512748130668081,-1.251882108722081,0.0,0.10957800910752284,-0.8707208703712623,0.37989140631742485,-2.408703410980144,-0.7089563385307599,0.709976295632011,0.028024955052427285]\n"
     ]
    }
   ],
   "source": [
    "# MLR mae model params\n",
    "best_mlr_mae_Pipeline = mlr_mae_cv_model.bestModel\n",
    "best_mlr_mae_Model = best_mlr_mae_Pipeline.stages[4]\n",
    "print(best_mlr_mae_Model.explainParam('elasticNetParam'))\n",
    "print(best_mlr_mae_Model.explainParam('regParam'))\n",
    "print(best_mlr_mae_Model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c0276-568a-4a9c-9e0f-a485ddb467a0",
   "metadata": {},
   "source": [
    "#### Cross Validation Selected Model Parameters     \n",
    "\n",
    "Using RMSE and MAE in cross-validation ended up selecting the same model, with a value of `0` for the `elasticNetParam` and a value of `10` for the `regParam`. Note that since `0` was selected as the optimal value for the `elasticNetParam` the selected model is a **ridge regression model**, with a penalty of `10`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f628019-168a-471a-8fd6-36ddfb6f46e8",
   "metadata": {},
   "source": [
    "#### Evaluating CV Selected Model on Training Set  \n",
    "\n",
    "`LinearRegression` in `MLlib` makes it easy to find the training RMSE and MAE values when we use this in cross validation. The fitted linear regression model has an attribute, `summary` that will allow you to easily access statistics on residuals, MSE and r-squared values for your fitted model. We will use this to print our values for RMSE and MAE for our training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2051be7b-7442-4371-95bf-483929c2813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MLR (Ridge Regression) Training RMSE: 11.923348668556025\n",
      "Best MLR (Ridge Regression) Training MAE: 5.102668265393184\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best MLR (Ridge Regression) Training RMSE: {best_mlr_Model.summary.rootMeanSquaredError}\")\n",
    "print(f\"Best MLR (Ridge Regression) Training MAE: {best_mlr_Model.summary.meanAbsoluteError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7dd16-76a0-4b7a-854f-812dc7ec4985",
   "metadata": {},
   "source": [
    "### Model 2  - Regression Tree  \n",
    "\n",
    "One benefit of regression trees is that there is not a need to scale the predictors in the model. As a result, we do not need to include this transformation in our regression tree pipeline. We still want to use our onehot encoder transformation to create dummy variables for the categorical variables in our model. We will use the `tree_features_Assembler` that we created above to create our features vector for our regression trees. We will be using `DecisionTreeRegressor` from `MLLib` for our model fitting.  \n",
    "\n",
    "**Model Parameters**  \n",
    "\n",
    "We will be finding optimal values for the `maxDepth` and `minInstancesPerNode` parameters using the `ParamGridBuilder()` and cross validation.  \n",
    "\n",
    "-  `maxDepth` - The `maxDepth` parameter represents the maximum depth of the tree.  \n",
    "    - Valid range of values is from 0 to 30.    \n",
    "    - Default value: `5`  \n",
    "    \n",
    "-  `minInstancesPerNode` - Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid.  \n",
    "    - Valid range of values is greater than or equal to 1.  \n",
    "    - Default value: `1`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2283a30b-99a7-4ed5-b28a-0c9ba09ab98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6547f0b4-71f3-48f6-9367-eef531384b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating regression tree regressor instance for pipeline\n",
    "single_tree_regressor = DecisionTreeRegressor(featuresCol = 'features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "088a9fa4-0960-4bd7-9cf9-75e5fbe2e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating single regression tree pipeline \n",
    "single_tree_pipeline = Pipeline(stages = [sql_trans_label, encoder_trans, tree_features_Assembler, single_tree_regressor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "de75eb3a-dd88-44af-82d8-426b10b561aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating single regression tree param grid for cross-validation\n",
    "single_tree_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(single_tree_regressor.maxDepth, [5, 10, 15, 20, 25, 30]) \\\n",
    "    .addGrid(single_tree_regressor.minInstancesPerNode, [3, 5, 10, 50, 100]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e933911-69d1-4301-a3fa-5967b9f2bf3c",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e1356211-1110-4265-b4eb-c408055e84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating single regression tree crossvalidator using the single_tree_pipleline, single_tree_paramgrid, and using RMSE as our metric\n",
    "single_tree_crossval = CrossValidator(estimator = single_tree_pipeline,\n",
    "                                      estimatorParamMaps = single_tree_paramGrid,\n",
    "                                      evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                                      numFolds=5,\n",
    "                                      seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6d52b2fd-b20f-4d28-85a0-76521ec8b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the single tree cross validator that we created\n",
    "single_tree_cv_model = single_tree_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e1581ee1-7c72-4a70-a827-2fce89ea5692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 5)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1, current: 10)\n"
     ]
    }
   ],
   "source": [
    "# Single Regression Tree model stats\n",
    "best_single_tree_Pipeline = single_tree_cv_model.bestModel\n",
    "best_single_tree_Model = best_single_tree_Pipeline.stages[3]\n",
    "print(best_single_tree_Model.explainParam('maxDepth'))\n",
    "print(best_single_tree_Model.explainParam('minInstancesPerNode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136b705-73bd-4f19-a342-b66cadb8c191",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e6fe11a4-88ed-4020-ab2b-8707d4f22e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating single regression tree mae crossvalidator using the single_tree_pipleline, single_tree_paramgrid, and using RMSE as our metric\n",
    "single_tree_mae_crossval = CrossValidator(estimator = single_tree_pipeline,\n",
    "                                          estimatorParamMaps = single_tree_paramGrid,\n",
    "                                          evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                                          numFolds=5,\n",
    "                                          seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ce299117-9b6b-43e2-b539-0c01e1e745d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the single tree mae cross validator that we created\n",
    "single_tree_mae_cv_model = single_tree_mae_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "07df18f3-80b2-45e1-8dbd-04d48943b0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 15)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1, current: 3)\n"
     ]
    }
   ],
   "source": [
    "# Single Regression Tree mae model stats\n",
    "best_single_tree_mae_Pipeline = single_tree_mae_cv_model.bestModel\n",
    "best_single_tree_mae_Model = best_single_tree_mae_Pipeline.stages[3]\n",
    "print(best_single_tree_mae_Model.explainParam('maxDepth'))\n",
    "print(best_single_tree_mae_Model.explainParam('minInstancesPerNode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a742a-f296-4833-a2bc-fbab3af84702",
   "metadata": {},
   "source": [
    "#### Cross-Validation Selected Model Parameters  \n",
    "\n",
    "Cross-validation using RMSE selected a decision tree model with `5` for `maxDepth` and `10` for `minInstancesPerNode`. Cross validation using MAE selected a decision tree model with `15` for `maxDepth` and `3` for `minInstancesPerNode`. We can use `.toDebugString` to get a full description of each model. This will allow us to compare the complexity of the model created using RMSE to the model created using MAE. We want to avoid overfitting our model on our training data, as this generally reduces the model's performance on data that was not used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "17432988-cf10-4144-84d9-8183c27583cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE CV regression tree model selected:\n",
      "\n",
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_90701c76fabd, depth=5, numNodes=21, numFeatures=52\n",
      "  If (feature 19 in {0.0})\n",
      "   If (feature 13 in {0.0})\n",
      "    If (feature 51 <= 190.5)\n",
      "     If (feature 0 in {1.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 0 not in {1.0})\n",
      "      If (feature 40 <= 16.5)\n",
      "       Predict: 6.1454545454545455\n",
      "      Else (feature 40 > 16.5)\n",
      "       Predict: 3.025\n",
      "    Else (feature 51 > 190.5)\n",
      "     Predict: 14.764705882352942\n",
      "   Else (feature 13 not in {0.0})\n",
      "    If (feature 39 <= 18.5)\n",
      "     Predict: 33.27272727272727\n",
      "    Else (feature 39 > 18.5)\n",
      "     If (feature 47 <= 0.5)\n",
      "      Predict: 3.2\n",
      "     Else (feature 47 > 0.5)\n",
      "      If (feature 38 <= 240.5)\n",
      "       Predict: 11.5\n",
      "      Else (feature 38 > 240.5)\n",
      "       Predict: 18.75\n",
      "  Else (feature 19 not in {0.0})\n",
      "   If (feature 49 <= 1.5)\n",
      "    If (feature 43 <= 96.5)\n",
      "     Predict: 15.0\n",
      "    Else (feature 43 > 96.5)\n",
      "     Predict: 40.0\n",
      "   Else (feature 49 > 1.5)\n",
      "    Predict: 6.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nRMSE CV regression tree model selected:\\n\\n{best_single_tree_Model.toDebugString}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1f8d2d43-37cb-4ba6-a07b-44b65684ac8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE CV regression tree model selected:\n",
      "\n",
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_90701c76fabd, depth=15, numNodes=143, numFeatures=52\n",
      "  If (feature 19 in {0.0})\n",
      "   If (feature 9 in {0.0})\n",
      "    If (feature 13 in {0.0})\n",
      "     If (feature 12 in {0.0})\n",
      "      If (feature 51 <= 190.5)\n",
      "       If (feature 1 in {0.0})\n",
      "        If (feature 0 in {1.0})\n",
      "         Predict: 0.0\n",
      "        Else (feature 0 not in {1.0})\n",
      "         If (feature 38 <= 264.0)\n",
      "          If (feature 26 in {0.0})\n",
      "           If (feature 22 in {0.0})\n",
      "            If (feature 10 in {0.0})\n",
      "             If (feature 18 in {0.0})\n",
      "              If (feature 11 in {0.0})\n",
      "               If (feature 6 in {0.0})\n",
      "                If (feature 14 in {0.0})\n",
      "                 Predict: 2.819277108433735\n",
      "                Else (feature 14 not in {0.0})\n",
      "                 Predict: 5.142857142857143\n",
      "               Else (feature 6 not in {0.0})\n",
      "                Predict: 6.75\n",
      "              Else (feature 11 not in {0.0})\n",
      "               If (feature 41 <= 29.5)\n",
      "                Predict: 9.8\n",
      "               Else (feature 41 > 29.5)\n",
      "                If (feature 30 in {0.0})\n",
      "                 Predict: 3.75\n",
      "                Else (feature 30 not in {0.0})\n",
      "                 Predict: 6.25\n",
      "             Else (feature 18 not in {0.0})\n",
      "              If (feature 32 in {1.0})\n",
      "               Predict: 4.0\n",
      "              Else (feature 32 not in {1.0})\n",
      "               If (feature 36 in {1.0})\n",
      "                Predict: 6.333333333333333\n",
      "               Else (feature 36 not in {1.0})\n",
      "                Predict: 8.0\n",
      "            Else (feature 10 not in {0.0})\n",
      "             If (feature 50 <= 67.5)\n",
      "              Predict: 13.333333333333334\n",
      "             Else (feature 50 > 67.5)\n",
      "              If (feature 43 <= 91.5)\n",
      "               Predict: 4.333333333333333\n",
      "              Else (feature 43 > 91.5)\n",
      "               Predict: 7.0\n",
      "           Else (feature 22 not in {0.0})\n",
      "            If (feature 51 <= 168.5)\n",
      "             Predict: 10.666666666666666\n",
      "            Else (feature 51 > 168.5)\n",
      "             If (feature 39 <= 15.5)\n",
      "              Predict: 8.0\n",
      "             Else (feature 39 > 15.5)\n",
      "              Predict: 6.333333333333333\n",
      "          Else (feature 26 not in {0.0})\n",
      "           If (feature 40 <= 13.5)\n",
      "            If (feature 36 in {0.0})\n",
      "             If (feature 35 in {1.0})\n",
      "              Predict: 6.4\n",
      "             Else (feature 35 not in {1.0})\n",
      "              Predict: 8.0\n",
      "            Else (feature 36 not in {0.0})\n",
      "             Predict: 10.666666666666666\n",
      "           Else (feature 40 > 13.5)\n",
      "            Predict: 5.6\n",
      "         Else (feature 38 > 264.0)\n",
      "          If (feature 7 in {0.0})\n",
      "           If (feature 14 in {0.0})\n",
      "            If (feature 41 <= 36.5)\n",
      "             If (feature 47 <= 0.5)\n",
      "              If (feature 43 <= 93.5)\n",
      "               Predict: 14.0\n",
      "              Else (feature 43 > 93.5)\n",
      "               If (feature 31 in {1.0})\n",
      "                Predict: 5.666666666666667\n",
      "               Else (feature 31 not in {1.0})\n",
      "                Predict: 8.0\n",
      "             Else (feature 47 > 0.5)\n",
      "              If (feature 23 in {1.0})\n",
      "               If (feature 42 <= 253.711)\n",
      "                Predict: 1.2\n",
      "               Else (feature 42 > 253.711)\n",
      "                Predict: 4.0\n",
      "              Else (feature 23 not in {1.0})\n",
      "               If (feature 36 in {1.0})\n",
      "                If (feature 22 in {1.0})\n",
      "                 Predict: 4.666666666666667\n",
      "                Else (feature 22 not in {1.0})\n",
      "                 Predict: 6.5\n",
      "               Else (feature 36 not in {1.0})\n",
      "                If (feature 42 <= 240.515)\n",
      "                 Predict: 6.818181818181818\n",
      "                Else (feature 42 > 240.515)\n",
      "                 Predict: 7.7\n",
      "            Else (feature 41 > 36.5)\n",
      "             If (feature 36 in {0.0})\n",
      "              If (feature 42 <= 265.31600000000003)\n",
      "               If (feature 33 in {1.0})\n",
      "                If (feature 42 <= 238.5325)\n",
      "                 Predict: 1.75\n",
      "                Else (feature 42 > 238.5325)\n",
      "                 Predict: 3.75\n",
      "               Else (feature 33 not in {1.0})\n",
      "                Predict: 6.666666666666667\n",
      "              Else (feature 42 > 265.31600000000003)\n",
      "               If (feature 42 <= 289.53499999999997)\n",
      "                If (feature 33 in {0.0})\n",
      "                 Predict: 1.4\n",
      "                Else (feature 33 not in {0.0})\n",
      "                 Predict: 1.6666666666666667\n",
      "               Else (feature 42 > 289.53499999999997)\n",
      "                If (feature 42 <= 307.46900000000005)\n",
      "                 Predict: 3.75\n",
      "                Else (feature 42 > 307.46900000000005)\n",
      "                 Predict: 2.0\n",
      "             Else (feature 36 not in {0.0})\n",
      "              Predict: 6.8\n",
      "           Else (feature 14 not in {0.0})\n",
      "            If (feature 30 in {0.0})\n",
      "             Predict: 7.666666666666667\n",
      "            Else (feature 30 not in {0.0})\n",
      "             Predict: 26.666666666666668\n",
      "          Else (feature 7 not in {0.0})\n",
      "           Predict: 20.0\n",
      "       Else (feature 1 not in {0.0})\n",
      "        If (feature 39 <= 25.5)\n",
      "         If (feature 38 <= 240.5)\n",
      "          Predict: 5.666666666666667\n",
      "         Else (feature 38 > 240.5)\n",
      "          Predict: 8.0\n",
      "        Else (feature 39 > 25.5)\n",
      "         Predict: 25.666666666666668\n",
      "      Else (feature 51 > 190.5)\n",
      "       If (feature 30 in {0.0})\n",
      "        If (feature 37 in {1.0})\n",
      "         Predict: 2.25\n",
      "        Else (feature 37 not in {1.0})\n",
      "         If (feature 42 <= 307.46900000000005)\n",
      "          Predict: 4.0\n",
      "         Else (feature 42 > 307.46900000000005)\n",
      "          Predict: 6.666666666666667\n",
      "       Else (feature 30 not in {0.0})\n",
      "        If (feature 42 <= 307.46900000000005)\n",
      "         Predict: 51.25\n",
      "        Else (feature 42 > 307.46900000000005)\n",
      "         Predict: 1.6666666666666667\n",
      "     Else (feature 12 not in {0.0})\n",
      "      If (feature 39 <= 26.5)\n",
      "       Predict: 43.666666666666664\n",
      "      Else (feature 39 > 26.5)\n",
      "       Predict: 2.6666666666666665\n",
      "    Else (feature 13 not in {0.0})\n",
      "     If (feature 41 <= 47.5)\n",
      "      If (feature 38 <= 290.0)\n",
      "       If (feature 47 <= 0.5)\n",
      "        If (feature 43 <= 95.5)\n",
      "         If (feature 40 <= 10.5)\n",
      "          If (feature 42 <= 289.53499999999997)\n",
      "           Predict: 3.5\n",
      "          Else (feature 42 > 289.53499999999997)\n",
      "           Predict: 4.0\n",
      "         Else (feature 40 > 10.5)\n",
      "          Predict: 10.666666666666666\n",
      "        Else (feature 43 > 95.5)\n",
      "         If (feature 43 <= 96.5)\n",
      "          Predict: 2.6666666666666665\n",
      "         Else (feature 43 > 96.5)\n",
      "          If (feature 42 <= 238.5325)\n",
      "           Predict: 1.6666666666666667\n",
      "          Else (feature 42 > 238.5325)\n",
      "           Predict: 1.3333333333333333\n",
      "       Else (feature 47 > 0.5)\n",
      "        If (feature 42 <= 319.99199999999996)\n",
      "         If (feature 39 <= 21.0)\n",
      "          Predict: 16.0\n",
      "         Else (feature 39 > 21.0)\n",
      "          Predict: 8.0\n",
      "        Else (feature 42 > 319.99199999999996)\n",
      "         If (feature 32 in {1.0})\n",
      "          Predict: 16.6\n",
      "         Else (feature 32 not in {1.0})\n",
      "          Predict: 26.666666666666668\n",
      "      Else (feature 38 > 290.0)\n",
      "       Predict: 40.25\n",
      "     Else (feature 41 > 47.5)\n",
      "      Predict: 47.0\n",
      "   Else (feature 9 not in {0.0})\n",
      "    Predict: 42.0\n",
      "  Else (feature 19 not in {0.0})\n",
      "   If (feature 49 <= 1.5)\n",
      "    If (feature 43 <= 98.5)\n",
      "     If (feature 40 <= 11.5)\n",
      "      Predict: 38.6\n",
      "     Else (feature 40 > 11.5)\n",
      "      If (feature 35 in {0.0})\n",
      "       If (feature 33 in {0.0})\n",
      "        If (feature 36 in {0.0})\n",
      "         Predict: 8.0\n",
      "        Else (feature 36 not in {0.0})\n",
      "         Predict: 10.666666666666666\n",
      "       Else (feature 33 not in {0.0})\n",
      "        Predict: 14.333333333333334\n",
      "      Else (feature 35 not in {0.0})\n",
      "       If (feature 43 <= 97.5)\n",
      "        Predict: 24.0\n",
      "       Else (feature 43 > 97.5)\n",
      "        Predict: 18.666666666666668\n",
      "    Else (feature 43 > 98.5)\n",
      "     Predict: 66.66666666666667\n",
      "   Else (feature 49 > 1.5)\n",
      "    If (feature 45 <= 1.5)\n",
      "     Predict: 8.0\n",
      "    Else (feature 45 > 1.5)\n",
      "     Predict: 4.333333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nMAE CV regression tree model selected:\\n\\n{best_single_tree_mae_Model.toDebugString}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c8da4-37b3-4aef-b2e9-67773f10aa40",
   "metadata": {},
   "source": [
    "When we look at the full model descriptions we can see that the model selected via mean absolute error is much more complex than what was selected using RMSE. To avoid overfitting our single regression tree model to our training set, we will use the model selected using RMSE during cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e9155-7a2d-4293-962b-525aa4a98896",
   "metadata": {},
   "source": [
    "#### Evaluating CV Selected Model on Training Set  \n",
    "\n",
    "Unfortunately, the `DecisionTreeRegerssionModel` in `MLlib` does not have the `summary` attribute, so we need a few steps to print the RMSE and MAE for our training set. We will use `RegressionEvaluator` to help get these values. First, we will use `single_tree_cv_model.transform(train)` to create predictions based on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "40929a8c-caef-4984-8e88-82dea07373cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_train = single_tree_cv_model.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3549b5-9d81-4342-b58a-75fa554b203a",
   "metadata": {},
   "source": [
    "Let's take a quick look at the predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7563ceb9-9a7c-4a42-8a16-543787c0799d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reason_for_absence</th>\n",
       "      <th>Day_of_the_week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation_expense</th>\n",
       "      <th>Distance_from_Residence_to_Work</th>\n",
       "      <th>Service_time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Workload_per_day</th>\n",
       "      <th>Hit_target</th>\n",
       "      <th>Disciplinary_failure</th>\n",
       "      <th>...</th>\n",
       "      <th>Social_smoker</th>\n",
       "      <th>Pet</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>label</th>\n",
       "      <th>Reason_onehot</th>\n",
       "      <th>Day_onehot</th>\n",
       "      <th>Season_onehot</th>\n",
       "      <th>features</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>37</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>172</td>\n",
       "      <td>8</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>6.145455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>163</td>\n",
       "      <td>8</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>6.145455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>163</td>\n",
       "      <td>8</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>6.145455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>246.288</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>2</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>24</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>246.288</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>237.656</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>2</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3.025000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Reason_for_absence  Day_of_the_week  Seasons  Transportation_expense  \\\n",
       "0                    22                2        1                     235   \n",
       "1                    18                2        1                     235   \n",
       "2                    18                5        1                     235   \n",
       "3                    11                4        1                     179   \n",
       "4                    11                2        1                     179   \n",
       "..                  ...              ...      ...                     ...   \n",
       "576                  14                5        3                     118   \n",
       "577                  19                5        3                     118   \n",
       "578                  19                5        3                     118   \n",
       "579                  23                3        3                     118   \n",
       "580                  23                2        3                     118   \n",
       "\n",
       "     Distance_from_Residence_to_Work  Service_time  Age  Workload_per_day  \\\n",
       "0                                 11            14   37           239.554   \n",
       "1                                 29            12   48           205.917   \n",
       "2                                 29            12   48           205.917   \n",
       "3                                 51            18   38           239.554   \n",
       "4                                 51            18   38           205.917   \n",
       "..                               ...           ...  ...               ...   \n",
       "576                               13            18   50           246.288   \n",
       "577                               13            18   50           275.089   \n",
       "578                               13            18   50           275.089   \n",
       "579                               13            18   50           246.288   \n",
       "580                               13            18   50           237.656   \n",
       "\n",
       "     Hit_target  Disciplinary_failure  ...  Social_smoker  Pet  Weight  \\\n",
       "0            97                     0  ...              0    1      88   \n",
       "1            92                     0  ...              1    5      88   \n",
       "2            92                     0  ...              1    5      88   \n",
       "3            97                     0  ...              0    0      89   \n",
       "4            92                     0  ...              0    0      89   \n",
       "..          ...                   ...  ...            ...  ...     ...   \n",
       "576          91                     0  ...              0    0      98   \n",
       "577          96                     0  ...              0    0      98   \n",
       "578          96                     0  ...              0    0      98   \n",
       "579          91                     0  ...              0    0      98   \n",
       "580          99                     0  ...              0    0      98   \n",
       "\n",
       "     Height  label                                      Reason_onehot  \\\n",
       "0       172      8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1       163      8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2       163      8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3       170      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4       170      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..      ...    ...                                                ...   \n",
       "576     178      2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "577     178      3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "578     178     24  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "579     178      3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "580     178      2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                         Day_onehot         Season_onehot  \\\n",
       "0    (0.0, 0.0, 1.0, 0.0, 0.0, 0.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "1    (0.0, 0.0, 1.0, 0.0, 0.0, 0.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "2    (0.0, 0.0, 0.0, 0.0, 0.0, 1.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "3    (0.0, 0.0, 0.0, 0.0, 1.0, 0.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "4    (0.0, 0.0, 1.0, 0.0, 0.0, 0.0)  (0.0, 1.0, 0.0, 0.0)   \n",
       "..                              ...                   ...   \n",
       "576  (0.0, 0.0, 0.0, 0.0, 0.0, 1.0)  (0.0, 0.0, 0.0, 1.0)   \n",
       "577  (0.0, 0.0, 0.0, 0.0, 0.0, 1.0)  (0.0, 0.0, 0.0, 1.0)   \n",
       "578  (0.0, 0.0, 0.0, 0.0, 0.0, 1.0)  (0.0, 0.0, 0.0, 1.0)   \n",
       "579  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0)  (0.0, 0.0, 0.0, 1.0)   \n",
       "580  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0)  (0.0, 0.0, 0.0, 1.0)   \n",
       "\n",
       "                                              features prediction  \n",
       "0    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   6.145455  \n",
       "1    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   6.145455  \n",
       "2    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   6.145455  \n",
       "3    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   3.025000  \n",
       "4    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   3.025000  \n",
       "..                                                 ...        ...  \n",
       "576  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   3.025000  \n",
       "577  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  15.000000  \n",
       "578  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  15.000000  \n",
       "579  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   3.025000  \n",
       "580  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   3.025000  \n",
       "\n",
       "[581 rows x 23 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srt_train.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c28aaa-800f-48d9-ac86-69eb06e024a6",
   "metadata": {},
   "source": [
    "#### Using `RegressionEvaluator()` On Training Set  \n",
    "\n",
    "We will first set up the `regression_eval` evaluator. This can be used for model evaluation going forward, there is no need to create this for each specific model we use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5b10cb93-9b42-417b-9c3a-fdabd19e3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use regression_eval for all of our regression evaluators going forward\n",
    "regression_eval = RegressionEvaluator(labelCol = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae68653-7dc1-4951-878d-8c5627d324f3",
   "metadata": {},
   "source": [
    "Now let's use `regression_eval` to return the training set RMSE and MAE for our single regression tree model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "96c13eae-b305-4217-ad19-8089d9fb5fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Regression Tree Training  RMSE: 11.742741107390424\n",
      "Single Regression Tree Training MAE: 5.075556158936739\n"
     ]
    }
   ],
   "source": [
    "# We can use regression_eval for all of our regression evaluators going forward\n",
    "regression_eval = RegressionEvaluator(labelCol = 'label')\n",
    "\n",
    "# Using regression_eval to return rmse and mae values for our training data\n",
    "srt_train_rmse = regression_eval.evaluate(srt_train, {regression_eval.metricName:'rmse'})\n",
    "srt_train_mae = regression_eval.evaluate(srt_train,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"Single Regression Tree Training  RMSE: {srt_train_rmse}\")\n",
    "print(f\"Single Regression Tree Training MAE: {srt_train_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c1208-dd62-447a-af13-a86c69d80e9a",
   "metadata": {},
   "source": [
    "So, we can see that our single tree regression model performed slightly better than our penalized MLR (Ridge Regression) model based on RMSE (11.74 vs 11.92) on our training set and MAE (5.08 vs 5.10). However, we really want to compare model performance on the test set, which we will do in our [Model Testing](#Model-Testing) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5af89-1dc9-481f-928a-bf0d33ac1e79",
   "metadata": {},
   "source": [
    "### Model 3 - Random Forest  \n",
    "\n",
    "We will use the same set of transformations in our random forest pipeline as we did with our single regression tree pipeline, this time using the `RandomForestRegressor` as our last stage in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "0e290c63-3aba-49b5-867a-7972b4f6bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4a609ee0-77f6-4040-bd73-1f270f04ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random forest regressor instance for pipeline\n",
    "rf_regressor = RandomForestRegressor(featuresCol = 'features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "33c5c4ee-bb43-4df9-8be8-8c6a7294595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random forest pipeline using all_vars_vectorAssembler and rf_regressor\n",
    "rf_pipeline = Pipeline(stages = [sql_trans_label, encoder_trans, tree_features_Assembler, rf_regressor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3ded6-9e3e-4309-8948-49440a761c60",
   "metadata": {},
   "source": [
    "#### Parameters for Random Forest Model  \n",
    "\n",
    "There are several possible parameters available for a building the random forest model using `RandomForestRegressor()`. We will specify values/possible values for three parameters, `numTrees`, `featureSubsetStrategy`, and `maxDepth`.  \n",
    "\n",
    "1.  The `numTrees` parameter - This parameter has a default value of `20` trees. By increasing this value we can bring down the variance in our model as compared to the single regression tree. We will set this value to `100` for our models.  \n",
    "2.  The `featureSubsetStrategy` parameter - This parameter is the number of features to consider for splits at each tree mode. We will build our parameter grid using `onethird`, `sqrt`, and `log2` as possible values for the `featureSubsetStrategy`. The sossible values for this parameter in `MLlib` are:\n",
    "    - `auto` - When this parameter is set to `auto` the number of features will automatically be selected based on the number of trees and the type of model. Our model is a regression model with `100` trees, and as a result `onethird` would be selected automatically. Note that `auto` is the default value for `featureSubsetStrategy`.  \n",
    "    - `all` - Use all features. If this is selected `RandomForestRegressor()` generates a bagged tree model as opposed to a random forest model.   \n",
    "    - `onethird` - The number of features to consider for splits at each tree node will be the total number of features in our model divided by three.  This is usually used for regression problems.  \n",
    "    - `sqrt` - This will use the square root of the number of features for the number of features to consider. This is usually used for classification problems.  \n",
    "    - `log2` - This will use the $log_2$ of the number of features for the number of features to consider.  \n",
    "    - `n` - Number of features  \n",
    "        - When `n` is in the range of 0 to 1 this will represent `n` times the number of features.  \n",
    "        - When `n` is in the range of 1 to the total number of features, this will represent `n` features to include at each split.  \n",
    "3.  The `maxDepth` parameter - This is the value for the maximum depth of the tree, and can have values ranging from `0` to `30`, with a default value of `5`. The possible values of `maxDepth` that we will use during model selection are `3`, `5`, `10`, `15`, `20`, and `25`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0c949d69-c6cd-4e7f-aeae-1eb91e019e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating param grid for cross validation\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_regressor.numTrees, [100]) \\\n",
    "    .addGrid(rf_regressor.featureSubsetStrategy, ['onethird', 'sqrt', 'log2']) \\\n",
    "    .addGrid(rf_regressor.maxDepth, [3, 5, 10, 15, 20, 25]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748d5cc-9842-456b-9710-d5dddc37d272",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "003d2657-4df1-47fb-ba35-1b38dbbe41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random forest crossvalidator\n",
    "rf_crossval = CrossValidator(estimator = rf_pipeline,\n",
    "                             estimatorParamMaps = rf_paramGrid,\n",
    "                             evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                             numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "154b6cec-6576-40dd-802d-16b2c98ea8bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting model using the random forest cross validator that we created\n",
    "rf_cv_model = rf_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a25b15ea-6fee-42da-961b-40f43afadfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE Random Forest Model Max Depth: 15\n",
      "Best RMSE Random Forest Model Feature Subset: sqrt\n"
     ]
    }
   ],
   "source": [
    "best_rf_Pipeline = rf_cv_model.bestModel\n",
    "best_rf_Model = best_rf_Pipeline.stages[3]\n",
    "print(f\"Best RMSE Random Forest Model Max Depth: {best_rf_Model.getMaxDepth()}\")\n",
    "print(f\"Best RMSE Random Forest Model Feature Subset: {best_rf_Model.getFeatureSubsetStrategy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f53bf-35ff-4a93-931d-4262c39109b8",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b4bfb130-0227-4b74-8a34-e63a37e81800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random forest mae crossvalidator\n",
    "rf_mae_crossval = CrossValidator(estimator = rf_pipeline,\n",
    "                                 estimatorParamMaps = rf_paramGrid,\n",
    "                                 evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                                 numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c20559e3-4b55-4364-afa7-47836422f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the random forest mae cross validator that we created\n",
    "rf_mae_cv_model = rf_mae_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "77b68053-dda3-4578-a44d-1653777ca692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE Random Forest Model Max Depth: 30\n",
      "Best MAE Random Forest Model Feature Subset: onethird\n"
     ]
    }
   ],
   "source": [
    "best_rf_mae_Pipeline = rf_mae_cv_model.bestModel\n",
    "best_rf_mae_Model = best_rf_mae_Pipeline.stages[3]\n",
    "print(f\"Best MAE Random Forest Model Max Depth: {best_rf_mae_Model.getMaxDepth()}\")\n",
    "print(f\"Best MAE Random Forest Model Feature Subset: {best_rf_mae_Model.getFeatureSubsetStrategy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e80ce112-3a8a-48f6-9cf4-09c391dcdf82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_list = ['ID',\n",
    " 'Reason_for_absence',\n",
    " 'Month_of_absence',\n",
    " 'Day_of_the_week',\n",
    " 'Seasons',\n",
    " 'Transportation_expense',\n",
    " 'Distance_from_Residence_to_Work',\n",
    " 'Service_time',\n",
    " 'Age',\n",
    " 'Work_load_Average/day_',\n",
    " 'Hit_target',\n",
    " 'Disciplinary_failure',\n",
    " 'Education',\n",
    " 'Son',\n",
    " 'Social_drinker',\n",
    " 'Social_smoker',\n",
    " 'Pet',\n",
    " 'Weight',\n",
    " 'Height',\n",
    " 'Body_mass_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930c31f-c525-4a9b-9d94-ee838fb7d6f2",
   "metadata": {},
   "source": [
    "#### Cross-Validation Selected Model Parameters  \n",
    "\n",
    "We can see that cross validation selected slightly different models based on RMSE vs MAE.  \n",
    "\n",
    "- The RMSE model selected `sqrt` for the `featureSubsetStrategy` and `15` as the `maxDepth`  \n",
    "- The MAE model selected `onethird` for the `featureSubsetStrategy` and `30` as the `maxDepth`  \n",
    "\n",
    "As discussed earlier, RMSE tends to penalize outliers more heavily than MAE. In this case, I want to build a model that will penalize outliers, so I will select the `rf_cv_model` that was selected based on the best RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e52259-a25e-48f8-8fb4-ff9bd41e7b44",
   "metadata": {},
   "source": [
    "#### Evaluating CV Selected Model on Training Set  \n",
    "\n",
    "Like the single regression tree, there are a couple of steps that we need to go through to get values for training RMSE and MAE. We will again use `RegressionEvaluator` to help get these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2e85d7df-f462-4319-b740-439bbf303917",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train = rf_cv_model.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114c388-3972-4b43-82db-aac13a06c5d1",
   "metadata": {},
   "source": [
    "Now, let's look at the training RMSE and MAE for the fitted random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "004deabb-aa34-40c5-b9ec-2d6ddc8585ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training  RMSE: 6.370646025700593\n",
      "Random Forest Training MAE: 2.7237058352186043\n"
     ]
    }
   ],
   "source": [
    "rf_train_rmse = regression_eval.evaluate(rf_train, {regression_eval.metricName:'rmse'})\n",
    "rf_train_mae = regression_eval.evaluate(rf_train,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"Random Forest Training  RMSE: {rf_train_rmse}\")\n",
    "print(f\"Random Forest Training MAE: {rf_train_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519cb1f-908c-4045-a39e-5013b80f7178",
   "metadata": {},
   "source": [
    "We can see that of the models so far, the random forest model (selected via cross-validation based on RMSE) has performed the best on the training data out of the model classes considered so far. This gives us an idea of how our model performed at predictions based on data that were used to train the model. We are most concerned with how our models generalize to data not included in training the model, and in the  [Model Testing](#Model-Testing) section we will see how each of our models performs on new data in order to determine the best model overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf1b8c-1825-41f5-867e-37b33618326e",
   "metadata": {},
   "source": [
    "### Model 4  - Gradient Boosted Regression Tree  \n",
    "\n",
    "We can use the same basic pipeline for the gradient boosted tree model as we did for our other tree models. For cross validation, we will set up a parameter grid based on values for the `maxDepth` and `maxIter` parameters. The `maxDepth` parameter specifies the maximum depth of each regression tree and can have values from `0` to `30`, with a default value of `5`. The `maxIter` parameter specifies the maximum number of iterations and must be set to a value greater than or equal to `0`, with a default value of `20`. We will use the default value for the shrinkage parameter, `stepSize`, of `0.1`. As noted earlier, `MLlib` also allows to use only a subset of features at each split, similar to a random forest model, using the `featureSubsetStrategy`. The default setting for this parameter is `all`, meaning all features will be considered at each split. We will use this default value for our boosted tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5718804c-13aa-4816-8b88-1a331a58ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d3732c55-0240-441f-8e11-acd4453dbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating gradient boosted tree regressor instance for pipeline\n",
    "gbt_regressor = GBTRegressor(featuresCol = 'features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a53382aa-19d2-4bed-bce7-63f6e175cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating gbt pipeline \n",
    "gbt_pipeline = Pipeline(stages = [sql_trans_label, encoder_trans, tree_features_Assembler, gbt_regressor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47321f3d-0578-4655-9aab-76abc24da1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating gbt param grid for cross validation\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt_regressor.maxDepth, [3, 5, 10]) \\\n",
    "    .addGrid(gbt_regressor.maxIter, [5, 10, 20]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c538272-7e02-4674-bb0a-6285cd746aca",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "491168f5-a7e9-4aa0-9045-eb42d5349973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating gbt crossvalidator using the gbt_pipleline, gbt_paramgrid, and using RMSE as our metric\n",
    "gbt_crossval = CrossValidator(estimator = gbt_pipeline,\n",
    "                              estimatorParamMaps = gbt_paramGrid,\n",
    "                              evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                              numFolds=5,\n",
    "                              seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8b7aae62-ac44-4c99-907a-3c3639ccfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the gbt cross validator that we created\n",
    "gbt_cv_model = gbt_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9123aa40-f3e8-4fef-8932-5e5d5ce3fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE GBT Model Max Depth: 3\n",
      "Best RMSE GBT Model Feature Subset: all\n",
      "Best RMSE GBT Model Max Iterations: 10\n"
     ]
    }
   ],
   "source": [
    "best_gbt_Pipeline = gbt_cv_model.bestModel\n",
    "best_gbt_Model = best_gbt_Pipeline.stages[3]\n",
    "print(f\"Best RMSE GBT Model Max Depth: {best_gbt_Model.getMaxDepth()}\")\n",
    "print(f\"Best RMSE GBT Model Feature Subset: {best_gbt_Model.getFeatureSubsetStrategy()}\")\n",
    "print(f\"Best RMSE GBT Model Max Iterations: {best_gbt_Model.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e61b59-f775-493c-8d7d-7c68e8895fd2",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using MAE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6cb9ca11-9dab-4bc2-932a-49cb1c848916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating gbt mae crossvalidator using the gbt_pipleline, gbt_paramgrid, and using MAE as our metric\n",
    "gbt_mae_crossval = CrossValidator(estimator = gbt_pipeline,\n",
    "                                  estimatorParamMaps = gbt_paramGrid,\n",
    "                                  evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                                  numFolds=5,\n",
    "                                  seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fc84d072-fe15-456e-a03d-93ec11c47d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the gbt mae cross validator that we created\n",
    "gbt_mae_cv_model = gbt_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cb5a7c26-45b4-4a9f-8ba2-5570977ac05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE GBT Model Max Depth: 3\n",
      "Best MAE GBT Model Feature Subset: all\n",
      "Best MAE GBT Model Max Iterations: 10\n"
     ]
    }
   ],
   "source": [
    "best_gbt_mae_Pipeline = gbt_mae_cv_model.bestModel\n",
    "best_gbt_mae_Model = best_gbt_mae_Pipeline.stages[3]\n",
    "print(f\"Best MAE GBT Model Max Depth: {best_gbt_mae_Model.getMaxDepth()}\")\n",
    "print(f\"Best MAE GBT Model Feature Subset: {best_gbt_mae_Model.getFeatureSubsetStrategy()}\")\n",
    "print(f\"Best MAE GBT Model Max Iterations: {best_gbt_mae_Model.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3612b8-8c7b-4736-a13c-2b7120aed02f",
   "metadata": {},
   "source": [
    "#### Cross-Validation Selected Model Parameters  \n",
    "\n",
    "Cross validation using RMSE and cross validation using MAE both selected a `maxDepth` of `3` and `maxIter` of `10`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c7ae5-2579-4d3c-a3db-e3c73ea01827",
   "metadata": {},
   "source": [
    "#### Evaluating CV Selected Model on Training Set  \n",
    "\n",
    "Like the single regression tree, and random forest models, there are a couple of steps that we need to go through to get values for training RMSE and MAE. We will again use `RegressionEvaluator` to help get these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "29bf0c0e-1093-4898-90e9-76666b931650",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train = gbt_cv_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0a5643b5-b0fb-4824-8639-4bdf8b672b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Training  RMSE: 8.76280715230066\n",
      "GBT Training MAE: 4.383497016058646\n"
     ]
    }
   ],
   "source": [
    "gbt_train_rmse = regression_eval.evaluate(gbt_train, {regression_eval.metricName:'rmse'})\n",
    "gbt_train_mae = regression_eval.evaluate(gbt_train,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"GBT Training  RMSE: {gbt_train_rmse}\")\n",
    "print(f\"GBT Training MAE: {gbt_train_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d694b-6dd2-4d36-9d1d-c6f14628576e",
   "metadata": {},
   "source": [
    "So we can see that our gradient boosted tree model performed slightly worse on the training set than our random forest model, but better than model 1 (MLR- ridge regression) and model 2 (single regression tree). Again, we want to compare our models based on the test set to determine which model is the best overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1fed4-fcb1-410d-bee6-ac43b3ac0a7f",
   "metadata": {},
   "source": [
    "### Model 5 - LASSO Regression Model  \n",
    "\n",
    "We can fit a lasso regression model by using `LinearRegression` and specifying a value of `1` for the `ealsticNetParam`. We can then do cross validation using a range of penalties for the `regParam` parameter. For this model all basic transformations will be the same as those for the MLR model above.  \n",
    "\n",
    "Here we will allow for more possible values for our `regParam` than we did in Model 1. Since we are setting the value of the `elasticNetParam` to `1` we will set more values for the `regParam` than we did in Model 1 without adding additional computational time to cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c9cde05-b48f-44ae-9df7-5ef3376e4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lasso regressor instance for pipeline\n",
    "lasso_regressor = LinearRegression(featuresCol = 'features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "807d47c3-e90d-4229-b42d-63f1846160f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lasso pipeline \n",
    "lasso_pipeline = Pipeline(stages = [sql_trans_label, scale_trans, encoder_trans, features_Assembler, lasso_regressor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da61c80d-119a-4bc0-b616-b6632cf350fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lasso param grid for cross validation - for LASSO eleasticNetParam should be set to 1\n",
    "lasso_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lasso_regressor.regParam, [0.01, 0.04, 0.07, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 4.0, 6.0, 10.0, 15.0]) \\\n",
    "    .addGrid(lasso_regressor.elasticNetParam, [1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3cc2a-0155-476f-b570-c0bb6a1b4d2d",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab2cadc6-c360-421c-a875-c307b4be1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lasso crossvalidator using the lasso_pipleline, lasso_paramgrid, and using RMSE as our metric\n",
    "lasso_crossval = CrossValidator(estimator = lasso_pipeline,\n",
    "                                estimatorParamMaps = lasso_paramGrid,\n",
    "                                evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                                numFolds=5,\n",
    "                                seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6cc45b5f-ca25-40ea-881e-5e5380250e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the lasso cross validator that we created\n",
    "lasso_cv_model = lasso_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "13f83d4c-8d6b-41ab-bb21-9f4d1344840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LASSO Training RMSE: 11.887524127956175\n",
      "Best LASSO Training MAE: 5.212544932916098\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.6)\n",
      "\n",
      "Best LASSO RMSE Model coefficients:\n",
      "[0.0,-0.2358422125411112,0.0,0.0,0.0,0.0,1.098019879798396,0.0,0.0,1.1902917349234123,-3.137390878118685,2.726253698394409,3.072749146134405,0.0,0.0,0.0,0.0,1.941462014745174,0.0,29.911172275326905,2.0326926011551776,2.7439072962967916,12.327946908194416,8.625872056097837,1.3813660208620984,0.0,0.0,0.0,0.0,12.294965214782032,0.0,0.0,0.0,-1.0587350435553557,0.0,0.0,0.0,0.0,0.0,0.0,0.6898009870940447,0.0,0.0,-1.2154331037181016,0.0,0.0,-0.346977334243171,0.0,0.0,-0.3878008401784424,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "# Lasso model stats\n",
    "best_lasso_Pipeline = lasso_cv_model.bestModel\n",
    "best_lasso_Model = best_lasso_Pipeline.stages[4]\n",
    "print(f\"Best LASSO Training RMSE: {best_lasso_Model.summary.rootMeanSquaredError}\")\n",
    "print(f\"Best LASSO Training MAE: {best_lasso_Model.summary.meanAbsoluteError}\")\n",
    "print(best_lasso_Model.explainParam('regParam'))\n",
    "print(f'\\nBest LASSO RMSE Model coefficients:\\n{best_lasso_Model.coefficients}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0fd9c-99bd-4a40-a7c3-423f25267676",
   "metadata": {},
   "source": [
    "#### Cross-Validation Using MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "61553af6-3fe0-45e6-8a0f-8e064db6b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lasso mae crossvalidator using the lasso_pipleline, lasso_paramgrid, and using MAE as our metric\n",
    "lasso_mae_crossval = CrossValidator(estimator = lasso_pipeline,\n",
    "                                    estimatorParamMaps = lasso_paramGrid,\n",
    "                                    evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                                    numFolds=5,\n",
    "                                    seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9190d477-1a3a-4960-a0f8-e40638710400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model using the lasso mae cross validator that we created\n",
    "lasso_mae_cv_model = lasso_mae_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "474a3067-37fa-49ad-a1ab-e0dedaa87990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LASSO MAE Training RMSE: 11.887524127956175\n",
      "Best LASSO MAE Training MAE: 5.212544932916098\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.6)\n",
      "\n",
      "Best LASSO MAE Model coefficients:\n",
      "[0.0,-0.2358422125411112,0.0,0.0,0.0,0.0,1.098019879798396,0.0,0.0,1.1902917349234123,-3.137390878118685,2.726253698394409,3.072749146134405,0.0,0.0,0.0,0.0,1.941462014745174,0.0,29.911172275326905,2.0326926011551776,2.7439072962967916,12.327946908194416,8.625872056097837,1.3813660208620984,0.0,0.0,0.0,0.0,12.294965214782032,0.0,0.0,0.0,-1.0587350435553557,0.0,0.0,0.0,0.0,0.0,0.0,0.6898009870940447,0.0,0.0,-1.2154331037181016,0.0,0.0,-0.346977334243171,0.0,0.0,-0.3878008401784424,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "# Lasso mae model stats\n",
    "best_lasso_mae_Pipeline = lasso_mae_cv_model.bestModel\n",
    "best_lasso_mae_Model = best_lasso_mae_Pipeline.stages[4]\n",
    "print(f\"Best LASSO MAE Training RMSE: {best_lasso_mae_Model.summary.rootMeanSquaredError}\")\n",
    "print(f\"Best LASSO MAE Training MAE: {best_lasso_mae_Model.summary.meanAbsoluteError}\")\n",
    "print(best_lasso_mae_Model.explainParam('regParam'))\n",
    "print(f'\\nBest LASSO MAE Model coefficients:\\n{best_lasso_mae_Model.coefficients}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3958cb-167d-4ccf-ae95-0a5eb2d9a3ab",
   "metadata": {},
   "source": [
    "#### Cross-Validation Selected Model Parameter  \n",
    "\n",
    "Both RMSE and MAE cross validation selected a value of `0.6` for the `regParam` parameter. The resulting models produced were the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3845fe-52ec-4333-a991-70abe7936e65",
   "metadata": {},
   "source": [
    "# Model Testing  \n",
    "\n",
    "In this section we will determine the best overall model for predicting the number of absentee hours. We will take each of our five selected models and use them to make predictions on our `test` set. We will then compare them using the model metrics RMSE and MAE to determine the model with the best generalizability and the lowest values for RMSE and MAE on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d469e69-4e3f-443e-942e-4c02abeb5dec",
   "metadata": {},
   "source": [
    "### Model 1 - MLR  \n",
    "\n",
    "The model selected through cross-validation above was a ridge regression model with a `regParam` value of `10` and an `elasticNetParam` of `0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bb034b97-dafb-4460-ba22-9c1df3a8a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our fitted model from cross validation on our training set to get test error on our test set \n",
    "mlr_pred = mlr_cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b6d19610-26e7-45ac-883c-ce6e689e0499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR RMSE: 11.819008749039375\n",
      "MLR MAE: 5.132187387597737\n"
     ]
    }
   ],
   "source": [
    "regression_eval = RegressionEvaluator(labelCol = 'label')\n",
    "MLR_rmse = regression_eval.evaluate(mlr_pred, {regression_eval.metricName:'rmse'})\n",
    "MLR_mae = regression_eval.evaluate(mlr_pred,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"MLR RMSE: {MLR_rmse}\")\n",
    "print(f\"MLR MAE: {MLR_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7771c9c-6750-4687-80fb-75b7cb7c411e",
   "metadata": {},
   "source": [
    "### Model 2 - Single Regression Tree  \n",
    "\n",
    "The single regression tree model selected through cross-validation above had parameter values of `5` for `maxDepth` and `10` for `minInstancesPerNode`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fedd05cf-074f-44e7-9584-fef47af855d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions for our test set based on the model single regression tree model selected using cv on our training set\n",
    "srt_pred = single_tree_cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4603dd8b-a95f-4421-abdb-c55787739169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Regression Tree TEST RMSE: 12.86916994105285\n",
      "Single Regression Tree TEST MAE: 5.881674402179398\n"
     ]
    }
   ],
   "source": [
    "srt_rmse = regression_eval.evaluate(srt_pred, {regression_eval.metricName:'rmse'})\n",
    "srt_mae = regression_eval.evaluate(srt_pred,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"Single Regression Tree TEST RMSE: {srt_rmse}\")\n",
    "print(f\"Single Regression Tree TEST MAE: {srt_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae124f7f-da34-42c7-86cc-977c223618d5",
   "metadata": {},
   "source": [
    "### Model 3 - Random Forest  \n",
    "\n",
    "The random forest model selected through cross-validation above had parameter values of `sqrt` for `featureSubsetStrategy` and a `maxDepth` of `15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d78f693a-7c0d-4e50-a283-4027758f9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions for our test set based on the random forest model selected using cv on our training set\n",
    "rf_pred = rf_cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "199c1495-b98c-4e15-8cbb-6a08b503e42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest TEST RMSE: 12.14834800677297\n",
      "Random Forest TEST MAE: 5.177339465686892\n"
     ]
    }
   ],
   "source": [
    "rf_rmse = regression_eval.evaluate(rf_pred, {regression_eval.metricName:'rmse'})\n",
    "rf_mae = regression_eval.evaluate(rf_pred,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"Random Forest TEST RMSE: {rf_rmse}\")\n",
    "print(f\"Random Forest TEST MAE: {rf_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff1a33-dcdc-47ce-998a-2e2a75eb6761",
   "metadata": {},
   "source": [
    "### Model 4 - Gradient Boosted Regression Tree   \n",
    "\n",
    "The gradient boosted regression tree model selected through cross-validation above had parameter values of `10` for `maxIter` and a value of `3` for `maxDepth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dc9bda97-5784-44b4-913a-9e1c999b94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions for our test set based on the boosted regression tree model selected using cv on our training set\n",
    "gbt_pred = gbt_cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8d499cad-baad-45a6-bc1b-92a4a428a884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Test  RMSE: 12.700587549568038\n",
      "GBT Test MAE: 5.499868065476387\n"
     ]
    }
   ],
   "source": [
    "gbt_rmse = regression_eval.evaluate(gbt_pred, {regression_eval.metricName:'rmse'})\n",
    "gbt_mae = regression_eval.evaluate(gbt_pred,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"GBT Test  RMSE: {gbt_rmse}\")\n",
    "print(f\"GBT Test MAE: {gbt_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4949f-6503-4427-8f86-c370edf65a4a",
   "metadata": {},
   "source": [
    "### Model 5 - LASSO Regression Model  \n",
    "\n",
    "The lasso regression model selected through cross-validation above had a value of `0.6` for the `regParam` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "df958f0b-ad58-4b16-967b-c3c1ad76a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions for our test set based on the LASSO model selected using cv on our training set\n",
    "lasso_pred = lasso_cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e419304d-0d91-4552-94b1-cff9c36d64cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Test  RMSE: 12.045559791956627\n",
      "Lasso Test MAE: 5.092076341968221\n"
     ]
    }
   ],
   "source": [
    "lasso_rmse = regression_eval.evaluate(lasso_pred, {regression_eval.metricName:'rmse'})\n",
    "lasso_mae = regression_eval.evaluate(lasso_pred,{regression_eval.metricName:'mae'})\n",
    "\n",
    "print(f\"Lasso Test  RMSE: {lasso_rmse}\")\n",
    "print(f\"Lasso Test MAE: {lasso_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca45fd-57ae-4427-853b-6d751dbeccc6",
   "metadata": {},
   "source": [
    "# Comparing Final Models  \n",
    "\n",
    "Let's create a dataframe to make comparison of the models a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c2794a70-08d8-4d03-b3da-b5766aa9cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = {'Model':['MLR_Ridge_Regression', 'Single_Regression_Tree', 'Random_Forest', 'Boosted_Tree', 'Lasso_Regression'],\n",
    "                     'RMSE':[MLR_rmse, srt_rmse, rf_rmse, gbt_rmse, lasso_rmse],\n",
    "                     'MAE':[MLR_mae, srt_mae, rf_mae, gbt_mae, lasso_mae]\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1195f0-bbf5-40e9-bcc6-b2e98ebf49f8",
   "metadata": {},
   "source": [
    "## Final Model Selection  \n",
    "\n",
    "We can see in the output below that the top two models based on performance on the test set were model 1, the ridge regression model, and model 5, the lasso regression model. Model 1 was the best model when judged by RMSE, while model 5 was the best when compared based on MAE. As discussed earlier, one reason to use RMSE is that it places a heavier penalty on outliers. In this case we will use RMSE as the ultimate measure of model performance and will declare Model 1, the ridge regression model, to be the top performer of the models created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5581e3d2-9235-4b67-9f5a-adccce90e7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLR_Ridge_Regression</td>\n",
       "      <td>11.819009</td>\n",
       "      <td>5.132187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso_Regression</td>\n",
       "      <td>12.045560</td>\n",
       "      <td>5.092076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>12.148348</td>\n",
       "      <td>5.177339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boosted_Tree</td>\n",
       "      <td>12.700588</td>\n",
       "      <td>5.499868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Single_Regression_Tree</td>\n",
       "      <td>12.869170</td>\n",
       "      <td>5.881674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model       RMSE       MAE\n",
       "0    MLR_Ridge_Regression  11.819009  5.132187\n",
       "4        Lasso_Regression  12.045560  5.092076\n",
       "2           Random_Forest  12.148348  5.177339\n",
       "3            Boosted_Tree  12.700588  5.499868\n",
       "1  Single_Regression_Tree  12.869170  5.881674"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_performance).sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "62106de2-f7b0-429e-8f5d-78d62768adbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso_Regression</td>\n",
       "      <td>12.045560</td>\n",
       "      <td>5.092076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLR_Ridge_Regression</td>\n",
       "      <td>11.819009</td>\n",
       "      <td>5.132187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>12.148348</td>\n",
       "      <td>5.177339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boosted_Tree</td>\n",
       "      <td>12.700588</td>\n",
       "      <td>5.499868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Single_Regression_Tree</td>\n",
       "      <td>12.869170</td>\n",
       "      <td>5.881674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model       RMSE       MAE\n",
       "4        Lasso_Regression  12.045560  5.092076\n",
       "0    MLR_Ridge_Regression  11.819009  5.132187\n",
       "2           Random_Forest  12.148348  5.177339\n",
       "3            Boosted_Tree  12.700588  5.499868\n",
       "1  Single_Regression_Tree  12.869170  5.881674"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_performance).sort_values('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850436a-a06d-4db2-ac0f-fe534717f2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e0f1f5c2-813b-4ec2-8dbb-d3b83d55e2c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+--------------------+\n",
      "| ID|Reason_for_absence|Month_of_absence|Day_of_the_week|Seasons|Transportation_expense|Distance_from_Residence_to_Work|Service_time|Age|Workload_per_day|Hit_target|Disciplinary_failure|Education|Son|Social_drinker|Social_smoker|Pet|Weight|Height|Body_mass_index|Absenteeism_time_in_hours|            features|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+--------------------+\n",
      "| 11|                26|               7|              3|      1|                   289|                             36|          13| 33|         239.554|        97|                   0|        1|  2|             1|            0|  1|    90|   172|             30|                        4|[11.0,26.0,7.0,3....|\n",
      "| 36|                 0|               7|              3|      1|                   118|                             13|          18| 50|         239.554|        97|                   1|        1|  1|             1|            0|  0|    98|   178|             31|                        0|[36.0,0.0,7.0,3.0...|\n",
      "|  3|                23|               7|              4|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|[3.0,23.0,7.0,4.0...|\n",
      "|  7|                 7|               7|              5|      1|                   279|                              5|          14| 39|         239.554|        97|                   0|        1|  2|             1|            1|  0|    68|   168|             24|                        4|[7.0,7.0,7.0,5.0,...|\n",
      "| 11|                23|               7|              5|      1|                   289|                             36|          13| 33|         239.554|        97|                   0|        1|  2|             1|            0|  1|    90|   172|             30|                        2|[11.0,23.0,7.0,5....|\n",
      "|  3|                23|               7|              6|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|[3.0,23.0,7.0,6.0...|\n",
      "| 10|                22|               7|              6|      1|                   361|                             52|           3| 28|         239.554|        97|                   0|        1|  1|             1|            0|  4|    80|   172|             27|                        8|[10.0,22.0,7.0,6....|\n",
      "| 20|                23|               7|              6|      1|                   260|                             50|          11| 36|         239.554|        97|                   0|        1|  4|             1|            0|  0|    65|   168|             23|                        4|[20.0,23.0,7.0,6....|\n",
      "| 14|                19|               7|              2|      1|                   155|                             12|          14| 34|         239.554|        97|                   0|        1|  2|             1|            0|  0|    95|   196|             25|                       40|[14.0,19.0,7.0,2....|\n",
      "|  1|                22|               7|              2|      1|                   235|                             11|          14| 37|         239.554|        97|                   0|        3|  1|             0|            0|  1|    88|   172|             29|                        8|[1.0,22.0,7.0,2.0...|\n",
      "| 20|                 1|               7|              2|      1|                   260|                             50|          11| 36|         239.554|        97|                   0|        1|  4|             1|            0|  0|    65|   168|             23|                        8|[20.0,1.0,7.0,2.0...|\n",
      "| 20|                 1|               7|              3|      1|                   260|                             50|          11| 36|         239.554|        97|                   0|        1|  4|             1|            0|  0|    65|   168|             23|                        8|[20.0,1.0,7.0,3.0...|\n",
      "| 20|                11|               7|              4|      1|                   260|                             50|          11| 36|         239.554|        97|                   0|        1|  4|             1|            0|  0|    65|   168|             23|                        8|[20.0,11.0,7.0,4....|\n",
      "|  3|                11|               7|              4|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        1|[3.0,11.0,7.0,4.0...|\n",
      "|  3|                23|               7|              4|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        4|[3.0,23.0,7.0,4.0...|\n",
      "| 24|                14|               7|              6|      1|                   246|                             25|          16| 41|         239.554|        97|                   0|        1|  0|             1|            0|  0|    67|   170|             23|                        8|[24.0,14.0,7.0,6....|\n",
      "|  3|                23|               7|              6|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        2|[3.0,23.0,7.0,6.0...|\n",
      "|  3|                21|               7|              2|      1|                   179|                             51|          18| 38|         239.554|        97|                   0|        1|  0|             1|            0|  0|    89|   170|             31|                        8|[3.0,21.0,7.0,2.0...|\n",
      "|  6|                11|               7|              5|      1|                   189|                             29|          13| 33|         239.554|        97|                   0|        1|  2|             0|            0|  2|    69|   167|             25|                        8|[6.0,11.0,7.0,5.0...|\n",
      "| 33|                23|               8|              4|      1|                   248|                             25|          14| 47|         205.917|        92|                   0|        1|  2|             0|            0|  1|    86|   165|             32|                        2|[33.0,23.0,8.0,4....|\n",
      "+---+------------------+----------------+---------------+-------+----------------------+-------------------------------+------------+---+----------------+----------+--------------------+---------+---+--------------+-------------+---+------+------+---------------+-------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a feature array by omitting the last column\n",
    "feature_cols = absentee_data.columns[:-1]\n",
    "vect_assembler = VectorAssembler(inputCols = feature_cols, outputCol=\"features\")\n",
    "#Utilize Assembler created above in order to add the feature column\n",
    "data_w_features = vect_assembler.transform(absentee_data)\n",
    "data_w_features.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
